[
  {
    "objectID": "docs/navigation.html",
    "href": "docs/navigation.html",
    "title": "Navigation: Traversing JSON Structure",
    "section": "",
    "text": "What you’ll learn: How to use goto(), each(), and goto_root() to navigate nested JSON structures and control iteration behavior.\nETL context: Navigation is the Extract step—it tells etielle where to find data in your JSON and how to iterate through it.",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#what-is-navigation",
    "href": "docs/navigation.html#what-is-navigation",
    "title": "Navigation: Traversing JSON Structure",
    "section": "What is Navigation?",
    "text": "What is Navigation?\nNavigation defines a path through your JSON structure. Think of it as giving directions: “Start at the ‘users’ key, then loop through each item in that array.”\nfrom etielle import etl, Field, TempField, get\n\nresult = (\n    etl(data)\n    .goto(\"users\")   # Navigate to the \"users\" key\n    .each()          # Iterate over each item\n    .map_to(table=\"users\", fields=[...])\n    .run()\n)",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#path-navigation-with-goto",
    "href": "docs/navigation.html#path-navigation-with-goto",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Path Navigation with goto()",
    "text": "Path Navigation with goto()\nThe goto() method navigates to nested locations in your JSON:\n\nfrom etielle import etl\n\n# Simple path: data[\"users\"]\netl(data).goto(\"users\")\n\n# Dot notation: data[\"response\"][\"data\"][\"users\"]\netl(data).goto(\"response.data.users\")\n\n# List syntax: data[\"pages\"][0][\"items\"]\netl(data).goto([\"pages\", 0, \"items\"])\n\n# Chained calls work too\netl(data).goto(\"response\").goto(\"data\").goto(\"users\")\n\n\nPath Syntax Options\n\n\n\nSyntax\nExample\nResult\n\n\n\n\nSingle key\ngoto(\"users\")\ndata[\"users\"]\n\n\nDot notation\ngoto(\"data.users\")\ndata[\"data\"][\"users\"]\n\n\nList of keys\ngoto([\"data\", \"users\"])\ndata[\"data\"][\"users\"]\n\n\nWith index\ngoto([\"pages\", 0])\ndata[\"pages\"][0]",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#iteration-with-each",
    "href": "docs/navigation.html#iteration-with-each",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Iteration with each()",
    "text": "Iteration with each()\nThe each() method iterates over items at the current path:\n\nFor lists: iterates by index (0, 1, 2, …)\nFor dicts: iterates key-value pairs\n\n\nfrom etielle import etl, Field, TempField, get, key\nimport json\n\n# Example: Iterating a dict by key-value pairs\ndata = {\n    \"settings\": {\n        \"theme\": \"dark\",\n        \"language\": \"en\",\n        \"notifications\": \"enabled\"\n    }\n}\n\nresult = (\n    etl(data)\n    .goto(\"settings\").each()  # Iterate key-value pairs\n    .map_to(table=\"settings\", fields=[\n        Field(\"name\", key()),       # \"theme\", \"language\", etc.\n        Field(\"value\", get([])),    # \"dark\", \"en\", etc. (empty path = current node)\n        TempField(\"name\", key())\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"settings\"].values()), indent=2))\n\n[\n  {\n    \"value\": \"dark\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"value\": \"en\",\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"value\": \"enabled\",\n    \"id\": \"__auto_2__\"\n  }\n]",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#nested-iteration",
    "href": "docs/navigation.html#nested-iteration",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Nested Iteration",
    "text": "Nested Iteration\nFor parent-child relationships (users -&gt; posts, orders -&gt; items), chain goto() and each():\n\nfrom etielle import etl, Field, TempField, get, get_from_parent\nimport json\n\ndata = {\n    \"users\": [\n        {\"id\": \"u1\", \"name\": \"Alice\", \"posts\": [\n            {\"id\": \"p1\", \"title\": \"Hello\"},\n            {\"id\": \"p2\", \"title\": \"World\"}\n        ]},\n        {\"id\": \"u2\", \"name\": \"Bob\", \"posts\": []}\n    ]\n}\n\nresult = (\n    etl(data)\n    # Outer iteration: users\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    # Inner iteration: posts within each user\n    .goto(\"posts\").each()\n    .map_to(table=\"posts\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"user_id\", get_from_parent(\"id\")),  # Link to parent user\n        Field(\"title\", get(\"title\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(\"Users:\", list(result.tables[\"users\"].values()))\nprint(\"Posts:\", list(result.tables[\"posts\"].values()))\n\nUsers: [{'name': 'Alice', 'id': '__auto_0__'}, {'name': 'Bob', 'id': '__auto_1__'}]\nPosts: [{'user_id': 'u1', 'title': 'Hello', 'id': '__auto_0__'}, {'user_id': 'u1', 'title': 'World', 'id': '__auto_1__'}]",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#deep-nesting-arbitrary-depth",
    "href": "docs/navigation.html#deep-nesting-arbitrary-depth",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Deep Nesting (Arbitrary Depth)",
    "text": "Deep Nesting (Arbitrary Depth)\nHandle arbitrarily deep structures with chained navigation. Use the depth parameter in get_from_parent() to access ancestors:\n\nget_from_parent(\"id\") or depth=1 -&gt; immediate parent\nget_from_parent(\"id\", depth=2) -&gt; grandparent\nget_from_parent(\"id\", depth=3) -&gt; great-grandparent\n\n\n# servers -&gt; channels -&gt; messages -&gt; reactions (3 levels deep)\nresult = (\n    etl(data)\n    .goto(\"servers\").each()\n    .map_to(table=\"servers\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    .goto(\"channels\").each()\n    .map_to(table=\"channels\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"server_id\", get_from_parent(\"id\")),  # depth=1\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    .goto(\"messages\").each()\n    .map_to(table=\"messages\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"channel_id\", get_from_parent(\"id\")),        # depth=1\n        Field(\"server_id\", get_from_parent(\"id\", depth=2)), # depth=2\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    .goto(\"reactions\").each()\n    .map_to(table=\"reactions\", fields=[\n        Field(\"emoji\", get(\"emoji\")),\n        Field(\"message_id\", get_from_parent(\"id\")),        # depth=1\n        Field(\"channel_id\", get_from_parent(\"id\", depth=2)), # depth=2\n        Field(\"server_id\", get_from_parent(\"id\", depth=3)),  # depth=3\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#multiple-json-roots-with-goto_root",
    "href": "docs/navigation.html#multiple-json-roots-with-goto_root",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Multiple JSON Roots with goto_root()",
    "text": "Multiple JSON Roots with goto_root()\nWhen processing multiple JSON objects, use goto_root(index) to switch between them:\n\nfrom etielle import etl, Field, TempField, get\n\nusers_json = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}]}\nprofiles_json = {\"profiles\": [{\"user_id\": \"u1\", \"bio\": \"Developer\"}]}\n\nresult = (\n    etl(users_json, profiles_json)  # Pass multiple JSON objects\n\n    # Process first root (index 0)\n    .goto_root(0)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    # Switch to second root (index 1)\n    .goto_root(1)\n    .goto(\"profiles\").each()\n    .map_to(table=\"users\", join_on=[\"id\"], fields=[\n        Field(\"bio\", get(\"bio\")),\n        TempField(\"id\", get(\"user_id\"))  # Matches users by id\n    ])\n\n    .run()\n)\n\nuser = list(result.tables[\"users\"].values())[0]\nprint(user)  # Has both name and bio merged together\n\n{'name': 'Alice', 'id': '__auto_0__'}\n\n\n\ngoto_root() Behavior\n\nResets the current navigation path\nResets the iteration state\nDefaults to index 0 if no argument provided\nRaises IndexError if index is out of range",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#resetting-navigation-with-goto_root",
    "href": "docs/navigation.html#resetting-navigation-with-goto_root",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Resetting Navigation with goto_root()",
    "text": "Resetting Navigation with goto_root()\nUse goto_root() (without arguments) to return to the root of the current JSON and start a new navigation path:\n\nfrom etielle import etl, Field, TempField, get\n\ndata = {\n    \"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}],\n    \"products\": [{\"id\": \"prod1\", \"name\": \"Widget\"}]\n}\n\nresult = (\n    etl(data)\n    # First path: users\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    # Reset to root, then navigate to products\n    .goto_root()\n    .goto(\"products\").each()\n    .map_to(table=\"products\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(\"Users:\", list(result.tables[\"users\"].values()))\nprint(\"Products:\", list(result.tables[\"products\"].values()))\n\nUsers: [{'name': 'Alice', 'id': '__auto_0__'}]\nProducts: [{'name': 'Widget', 'id': '__auto_0__'}]",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#row-merging-with-join_on",
    "href": "docs/navigation.html#row-merging-with-join_on",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Row Merging with join_on",
    "text": "Row Merging with join_on\nWhen multiple map_to() calls emit to the same table, rows with matching keys are merged:\n\nfrom etielle import etl, Field, TempField, get\n\ndata = {\n    \"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}],\n    \"profiles\": [{\"user_id\": \"u1\", \"email\": \"alice@example.com\"}]\n}\n\nresult = (\n    etl(data)\n    # First emission: basic user data\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))  # Key: u1\n    ])\n\n    # Second emission: add profile data to same table\n    .goto_root()\n    .goto(\"profiles\").each()\n    .map_to(table=\"users\", join_on=[\"id\"], fields=[  # join_on required for second emission\n        Field(\"email\", get(\"email\")),\n        TempField(\"id\", get(\"user_id\"))  # Key: u1 (matches above)\n    ])\n    .run()\n)\n\nuser = list(result.tables[\"users\"].values())[0]\nprint(user)  # {\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"alice@example.com\"}\n\n{'name': 'Alice', 'id': '__auto_0__'}\n\n\n\njoin_on Rules\n\nFirst map_to for a table doesn’t need join_on\nSubsequent map_to calls for the same table require join_on\njoin_on references field names (from Field or TempField)\nRows with matching keys are merged",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#navigation-reference",
    "href": "docs/navigation.html#navigation-reference",
    "title": "Navigation: Traversing JSON Structure",
    "section": "Navigation Reference",
    "text": "Navigation Reference\n\n\n\n\n\n\n\n\nMethod\nPurpose\nExample\n\n\n\n\ngoto(path)\nNavigate to a nested path\n.goto(\"users\") or .goto(\"data.users\")\n\n\neach()\nIterate over items at current path\n.goto(\"users\").each()\n\n\ngoto_root(index)\nSwitch to a different JSON root\n.goto_root(1) for second root\n\n\ngoto_root()\nReset to root of current JSON\n.goto_root() to start fresh",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/navigation.html#see-also",
    "href": "docs/navigation.html#see-also",
    "title": "Navigation: Traversing JSON Structure",
    "section": "See also",
    "text": "See also\n\nTransforms - Extracting values at each navigation position\nMapping Tables - Defining output structure with Field and TempField",
    "crumbs": [
      "Core Concepts",
      "Navigation"
    ]
  },
  {
    "objectID": "docs/database-loading.html",
    "href": "docs/database-loading.html",
    "title": "Database Loading: Persisting with load().run()",
    "section": "",
    "text": "What you’ll learn: How to use load() to persist your transformed data to a database using SQLAlchemy or SQLModel.\nETL context: Database loading is the Load step—the final step that persists your in-memory objects to a database.",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#what-is-load",
    "href": "docs/database-loading.html#what-is-load",
    "title": "Database Loading: Persisting with load().run()",
    "section": "What is load()?",
    "text": "What is load()?\nThe load(session) method configures your pipeline to persist results to a database. When combined with run(), it:\n\nBuilds all instances in memory\nBinds relationships\nAdds instances to the session\nFlushes (but does NOT commit)\n\nYou control the transaction (commit/rollback).\nfrom etielle import etl, Field, TempField, get\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[...])\n    .load(session)  # Configure database session\n    .run()          # Execute and flush\n)\n\nsession.commit()    # You control the transaction",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#basic-database-persistence",
    "href": "docs/database-loading.html#basic-database-persistence",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Basic Database Persistence",
    "text": "Basic Database Persistence\n\nSQLModel Example\n\nfrom sqlmodel import SQLModel, Field as SQLField, Session, create_engine\nfrom etielle import etl, Field, TempField, get\n\n# Define your model\nclass User(SQLModel, table=True):\n    id: str = SQLField(primary_key=True)\n    name: str\n    email: str | None = None\n\n# Create engine and tables\nengine = create_engine(\"sqlite:///example.db\")\nSQLModel.metadata.create_all(engine)\n\n# Your JSON data\ndata = {\n    \"users\": [\n        {\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"alice@example.com\"},\n        {\"id\": \"u2\", \"name\": \"Bob\", \"email\": \"bob@example.com\"}\n    ]\n}\n\n# Run pipeline with database loading\nwith Session(engine) as session:\n    result = (\n        etl(data)\n        .goto(\"users\").each()\n        .map_to(table=User, fields=[\n            Field(\"id\", get(\"id\")),\n            Field(\"name\", get(\"name\")),\n            Field(\"email\", get(\"email\")),\n            TempField(\"id\", get(\"id\"))\n        ])\n        .load(session)\n        .run()\n    )\n\n    session.commit()  # Persist to database\n\n# Query back\nwith Session(engine) as session:\n    users = session.query(User).all()\n    for user in users:\n        print(f\"{user.name}: {user.email}\")\n\n\n\nSQLAlchemy Example\n\nfrom sqlalchemy import Column, String, create_engine\nfrom sqlalchemy.orm import declarative_base, Session\nfrom etielle import etl, Field, TempField, get\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n    id = Column(String, primary_key=True)\n    name = Column(String)\n    email = Column(String)\n\nengine = create_engine(\"sqlite:///example.db\")\nBase.metadata.create_all(engine)\n\ndata = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"alice@example.com\"}]}\n\nwith Session(engine) as session:\n    result = (\n        etl(data)\n        .goto(\"users\").each()\n        .map_to(table=User, fields=[\n            Field(\"id\", get(\"id\")),\n            Field(\"name\", get(\"name\")),\n            Field(\"email\", get(\"email\")),\n            TempField(\"id\", get(\"id\"))\n        ])\n        .load(session)\n        .run()\n    )\n\n    session.commit()",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#transaction-control",
    "href": "docs/database-loading.html#transaction-control",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Transaction Control",
    "text": "Transaction Control\n\nYou Control Commit/Rollback\nload().run() flushes but does NOT commit. This gives you full control:\nwith Session(engine) as session:\n    try:\n        result = (\n            etl(data)\n            .goto(\"users\").each()\n            .map_to(table=User, fields=[...])\n            .load(session)\n            .run()\n        )\n\n        # Do additional work...\n        session.execute(...)\n\n        session.commit()  # Commit everything together\n\n    except Exception:\n        session.rollback()  # Rollback on error\n        raise\n\n\nBatch Processing\nProcess multiple JSON payloads in one transaction:\nwith Session(engine) as session:\n    for json_payload in payloads:\n        result = (\n            etl(json_payload)\n            .goto(\"users\").each()\n            .map_to(table=User, fields=[...])\n            .load(session)\n            .run()\n        )\n\n    session.commit()  # Commit all batches together",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#relationships-and-database-loading",
    "href": "docs/database-loading.html#relationships-and-database-loading",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Relationships and Database Loading",
    "text": "Relationships and Database Loading\nWhen using link_to() with ORM models, relationships are bound before flushing:\n\nfrom sqlmodel import SQLModel, Field as SQLField, Relationship, Session, create_engine\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\nclass User(SQLModel, table=True):\n    id: str = SQLField(primary_key=True)\n    name: str\n    posts: list[\"Post\"] = Relationship(back_populates=\"user\")\n\nclass Post(SQLModel, table=True):\n    id: str = SQLField(primary_key=True)\n    title: str\n    user_id: str = SQLField(foreign_key=\"user.id\")\n    user: User | None = Relationship(back_populates=\"posts\")\n\nengine = create_engine(\"sqlite:///example.db\")\nSQLModel.metadata.create_all(engine)\n\ndata = {\n    \"users\": [{\n        \"id\": \"u1\",\n        \"name\": \"Alice\",\n        \"posts\": [\n            {\"id\": \"p1\", \"title\": \"Hello\"},\n            {\"id\": \"p2\", \"title\": \"World\"}\n        ]\n    }]\n}\n\nwith Session(engine) as session:\n    result = (\n        etl(data)\n        .goto(\"users\").each()\n        .map_to(table=User, fields=[\n            Field(\"id\", get(\"id\")),\n            Field(\"name\", get(\"name\")),\n            TempField(\"id\", get(\"id\"))\n        ])\n\n        .goto(\"posts\").each()\n        .map_to(table=Post, fields=[\n            Field(\"id\", get(\"id\")),\n            Field(\"title\", get(\"title\")),\n            Field(\"user_id\", get_from_parent(\"id\")),  # Foreign key column\n            TempField(\"id\", get(\"id\")),\n            TempField(\"user_id\", get_from_parent(\"id\"))\n        ])\n        .link_to(User, by={\"user_id\": \"id\"})  # Bind relationship\n        .load(session)\n        .run()\n    )\n\n    session.commit()\n\n# Query with relationships\nwith Session(engine) as session:\n    user = session.get(User, \"u1\")\n    print(f\"{user.name} has {len(user.posts)} posts\")",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#upsert-behavior",
    "href": "docs/database-loading.html#upsert-behavior",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Upsert Behavior",
    "text": "Upsert Behavior\netielle uses “upsert” semantics—if a row with the same key already exists, it’s updated:\n# First load: creates user u1\ndata1 = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"old@example.com\"}]}\netl(data1).goto(\"users\").each().map_to(...).load(session).run()\n\n# Second load: updates user u1\ndata2 = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"new@example.com\"}]}\netl(data2).goto(\"users\").each().map_to(...).load(session).run()\n\nsession.commit()\n# User u1 now has email \"new@example.com\"\n\nMerge Policies with Database Loading\nMerge policies work with database loading too:\nfrom etielle import AddPolicy\n\n# Each load adds to the count\n.map_to(table=Stats, fields=[\n    Field(\"count\", literal(1), merge=AddPolicy()),\n    TempField(\"id\", get(\"id\"))\n])\n.load(session)\n.run()",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#without-database-loading",
    "href": "docs/database-loading.html#without-database-loading",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Without Database Loading",
    "text": "Without Database Loading\nYou can use etielle without database loading for in-memory transformations:\n# No load() = no database interaction\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[...])\n    .run()  # Just run(), no load()\n)\n\n# Result contains Pydantic/dataclass instances in memory\nfor key, user in result.tables[User].items():\n    print(user.name)  # Access in-memory objects",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#error-handling-during-load",
    "href": "docs/database-loading.html#error-handling-during-load",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Error Handling During Load",
    "text": "Error Handling During Load\n\nCheck Errors Before Commit\nwith Session(engine) as session:\n    result = (\n        etl(data)\n        .goto(\"users\").each()\n        .map_to(table=User, fields=[...])\n        .load(session)\n        .run()\n    )\n\n    if result.errors:\n        # Log or handle errors\n        for table, errors in result.errors.items():\n            for key, messages in errors.items():\n                print(f\"Error in {table}[{key}]: {messages}\")\n\n        session.rollback()\n    else:\n        session.commit()\n\n\nFail Fast Mode\nUse errors=\"fail_fast\" to raise immediately on validation errors:\ntry:\n    result = (\n        etl(data, errors=\"fail_fast\")\n        .goto(\"users\").each()\n        .map_to(table=User, fields=[...])\n        .load(session)\n        .run()\n    )\n    session.commit()\nexcept ValueError as e:\n    print(f\"Validation failed: {e}\")\n    session.rollback()",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#load-reference",
    "href": "docs/database-loading.html#load-reference",
    "title": "Database Loading: Persisting with load().run()",
    "section": "load() Reference",
    "text": "load() Reference\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nsession\nSQLAlchemy/SQLModel Session\nThe database session to use\n\n\n\n\nBehavior\n\nAll ORM instances are added to the session via session.add()\nsession.flush() is called to send SQL to the database\nTransaction is NOT committed—you must call session.commit()\nPlain dict results (from table=\"name\") are NOT added to session\n\n\n\nRequirements\n\nModel classes must be ORM models (have __tablename__)\nSession must be active and not in an error state\nModel’s table must exist in the database",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#best-practices",
    "href": "docs/database-loading.html#best-practices",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse Context Managers\nwith Session(engine) as session:\n    result = etl(data)...load(session).run()\n    session.commit()\n# Session is automatically closed\n\n\nHandle Large Datasets in Batches\nBATCH_SIZE = 1000\n\nwith Session(engine) as session:\n    for i in range(0, len(records), BATCH_SIZE):\n        batch = {\"users\": records[i:i + BATCH_SIZE]}\n        etl(batch).goto(\"users\").each().map_to(...).load(session).run()\n\n    session.commit()\n\n\nSeparate Transform and Load\nFor complex pipelines, consider separating concerns:\n# Transform step (no database)\nresult = etl(data).goto(\"users\").each().map_to(table=User, ...).run()\n\n# Validate\nif result.errors:\n    handle_errors(result.errors)\n    return\n\n# Load step\nwith Session(engine) as session:\n    for user in result.tables[User].values():\n        session.add(user)\n    session.commit()",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#supabase",
    "href": "docs/database-loading.html#supabase",
    "title": "Database Loading: Persisting with load().run()",
    "section": "Supabase",
    "text": "Supabase\netielle also supports loading data directly to Supabase using the same .load().run() pattern.\n\nBasic Supabase Example\n\nfrom supabase import create_client\nfrom etielle import etl, Field, TempField, get\n\n# Create Supabase client\nclient = create_client(\"https://your-project.supabase.co\", \"your-anon-key\")\n\ndata = {\n    \"users\": [\n        {\"id\": \"u1\", \"name\": \"Alice\"},\n        {\"id\": \"u2\", \"name\": \"Bob\"}\n    ]\n}\n\n# Load directly to Supabase\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .load(client)  # Supabase client auto-detected\n    .run()\n)\n\n\n\nSupabase with Relationships\n\nfrom supabase import create_client\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\nclient = create_client(\"https://your-project.supabase.co\", \"your-anon-key\")\n\ndata = {\n    \"users\": [{\n        \"id\": \"u1\",\n        \"name\": \"Alice\",\n        \"posts\": [\n            {\"id\": \"p1\", \"title\": \"Hello\"},\n            {\"id\": \"p2\", \"title\": \"World\"}\n        ]\n    }]\n}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    .goto(\"posts\").each()\n    .map_to(table=\"posts\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"title\", get(\"title\")),\n        Field(\"user_id\", get_from_parent(\"id\")),  # FK set via transform\n        TempField(\"id\", get(\"id\")),\n        TempField(\"user_id\", get_from_parent(\"id\"))\n    ])\n    .link_to(\"users\", by={\"user_id\": \"id\"})  # Defines insert order\n    .load(client)\n    .run()\n)\n\nTables are inserted in dependency order (parents before children) based on link_to() declarations.\n\n\nUpsert Mode\nUse upsert=True for idempotent loads. By default, Supabase uses the table’s primary key for conflict detection:\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[...])\n    .load(client, upsert=True)  # Uses table's primary key\n    .run()\n)\n\n\nCustom Conflict Columns\nUse upsert_on to specify which columns to use for conflict detection per table. This is useful when you want to upsert on a unique constraint other than the primary key:\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"email\", get(\"email\")),\n        Field(\"name\", get(\"name\")),\n    ])\n    .goto_root()\n    .goto(\"posts\").each()\n    .map_to(table=\"posts\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"user_id\", get(\"user_id\")),\n        Field(\"slug\", get(\"slug\")),\n        Field(\"title\", get(\"title\")),\n    ])\n    .load(client, upsert=True, upsert_on={\n        \"users\": \"email\",              # Upsert on email column\n        \"posts\": [\"user_id\", \"slug\"],  # Composite unique constraint\n    })\n    .run()\n)\nTables not in upsert_on use the default (table’s primary key).\n\n\nBatching\nControl batch size for large datasets:\n.load(client, batch_size=500)  # Default is 1000\n\n\nSupabase vs SQLAlchemy\n\n\n\nFeature\nSQLAlchemy/SQLModel\nSupabase\n\n\n\n\nTransaction\nSession-based\nPer-table HTTP calls\n\n\nRelationships\nORM object references\nFK columns via transforms\n\n\nCommit\nManual session.commit()\nAutomatic on insert\n\n\nUpsert\nORM merge\nupsert=True option\n\n\n\nKey difference: With Supabase, foreign key values must be set via transforms (like get_from_parent()) since there’s no ORM session to resolve relationships.\n\n\nLimitations\n\nNo transactions across tables: Each table insert is a separate HTTP call\nCaller-provided IDs required: DB-generated IDs require multiple round-trips (not yet supported)",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/database-loading.html#see-also",
    "href": "docs/database-loading.html#see-also",
    "title": "Database Loading: Persisting with load().run()",
    "section": "See also",
    "text": "See also\n\nMapping Tables - Defining output structure\nRelationships - Linking tables before persistence\nError Handling - Handling validation errors",
    "crumbs": [
      "Advanced Topics",
      "Database Loading"
    ]
  },
  {
    "objectID": "docs/transforms.html",
    "href": "docs/transforms.html",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "",
    "text": "What you’ll learn: How to use transforms to extract, format, and combine values from your JSON data.\nETL context: Transforms are part of the Transform step—they define how to compute field values from the current position in your JSON.",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#what-is-a-transform",
    "href": "docs/transforms.html#what-is-a-transform",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "What is a Transform?",
    "text": "What is a Transform?\nA transform is a function that extracts or computes a value from the current context. They’re “lazy”—they don’t run until the pipeline executes, and they adapt to wherever they’re used.\nfrom etielle import get, literal, concat\n\n# Extract a field\nField(\"name\", get(\"name\"))  # Get \"name\" from current node\n\n# Use a constant\nField(\"version\", literal(1))  # Always returns 1\n\n# Combine values\nField(\"full_name\", concat(get(\"first\"), literal(\" \"), get(\"last\")))",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#core-transforms",
    "href": "docs/transforms.html#core-transforms",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "Core Transforms",
    "text": "Core Transforms\n\nget(path) - Extract from current node\nThe most common transform. Extracts a value from the current JSON node.\n\nfrom etielle import etl, Field, TempField, get\nimport json\n\ndata = {\"users\": [{\"id\": \"u1\", \"profile\": {\"name\": \"Alice\", \"age\": 30}}]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),              # Simple field\n        Field(\"name\", get(\"profile.name\")),  # Nested with dot notation\n        Field(\"age\", get([\"profile\", \"age\"])),  # Nested with list\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"users\"].values()), indent=2))\n\n[\n  {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"id\": \"__auto_0__\"\n  }\n]\n\n\n\n\nget_from_parent(path, depth=1) - Extract from ancestor\nGets a value from a parent context. Essential for linking child records to parents.\n\nfrom etielle import etl, Field, TempField, get, get_from_parent\nimport json\n\ndata = {\n    \"users\": [{\n        \"id\": \"u1\",\n        \"name\": \"Alice\",\n        \"posts\": [{\"id\": \"p1\", \"title\": \"Hello\"}]\n    }]\n}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n\n    .goto(\"posts\").each()\n    .map_to(table=\"posts\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"title\", get(\"title\")),\n        Field(\"user_id\", get_from_parent(\"id\")),   # Get id from parent user\n        Field(\"user_name\", get_from_parent(\"name\")),  # Get name from parent\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"posts\"].values()), indent=2))\n\n[\n  {\n    \"title\": \"Hello\",\n    \"user_id\": \"u1\",\n    \"user_name\": \"Alice\",\n    \"id\": \"__auto_0__\"\n  }\n]\n\n\n\nDepth parameter\nUse depth to access grandparents and beyond:\nget_from_parent(\"id\")           # Parent (depth=1, default)\nget_from_parent(\"id\", depth=2)  # Grandparent\nget_from_parent(\"id\", depth=3)  # Great-grandparent\n\n\n\nget_from_root(path) - Extract from JSON root\nGets a value from the top-level JSON object, regardless of current position.\n\nfrom etielle import etl, Field, TempField, get, get_from_root\nimport json\n\ndata = {\n    \"version\": \"2.0\",\n    \"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}]\n}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        Field(\"api_version\", get_from_root(\"version\")),  # From root\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"users\"].values()), indent=2))\n\n[\n  {\n    \"name\": \"Alice\",\n    \"api_version\": \"2.0\",\n    \"id\": \"__auto_0__\"\n  }\n]\n\n\n\n\nliteral(value) - Constant value\nReturns a constant value, useful for defaults or computed fields.\n\nfrom etielle import etl, Field, TempField, get, literal\nimport json\n\ndata = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        Field(\"status\", literal(\"active\")),  # Constant value\n        Field(\"count\", literal(0)),          # Numeric constant\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"users\"].values()), indent=2))\n\n[\n  {\n    \"name\": \"Alice\",\n    \"status\": \"active\",\n    \"count\": 0,\n    \"id\": \"__auto_0__\"\n  }\n]",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#iteration-transforms",
    "href": "docs/transforms.html#iteration-transforms",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "Iteration Transforms",
    "text": "Iteration Transforms\n\nkey() - Current dict key\nWhen iterating over a dict with each(), returns the current key.\n\nfrom etielle import etl, Field, TempField, get, key\nimport json\n\ndata = {\n    \"scores\": {\n        \"alice\": 95,\n        \"bob\": 87,\n        \"carol\": 92\n    }\n}\n\nresult = (\n    etl(data)\n    .goto(\"scores\").each()  # Iterate dict key-value pairs\n    .map_to(table=\"scores\", fields=[\n        Field(\"player\", key()),       # \"alice\", \"bob\", \"carol\"\n        Field(\"score\", get([])),      # 95, 87, 92 (empty path = current node)\n        TempField(\"player\", key())\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"scores\"].values()), indent=2))\n\n[\n  {\n    \"score\": 95,\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"score\": 87,\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"score\": 92,\n    \"id\": \"__auto_2__\"\n  }\n]\n\n\n\n\nindex() - Current list index\nWhen iterating over a list with each(), returns the current index.\n\nfrom etielle import etl, Field, TempField, get, index\nimport json\n\ndata = {\"items\": [\"apple\", \"banana\", \"cherry\"]}\n\nresult = (\n    etl(data)\n    .goto(\"items\").each()\n    .map_to(table=\"items\", fields=[\n        Field(\"position\", index()),   # 0, 1, 2\n        Field(\"name\", get([])),       # \"apple\", \"banana\", \"cherry\"\n        TempField(\"position\", index())\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"items\"].values()), indent=2))\n\n[\n  {\n    \"name\": \"apple\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"name\": \"banana\",\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"name\": \"cherry\",\n    \"id\": \"__auto_2__\"\n  }\n]\n\n\n\n\nparent_key(depth=1) - Parent’s dict key\nGets the dict key from a parent iteration.\n\nfrom etielle import etl, Field, TempField, get, parent_key\nimport json\n\ndata = {\n    \"categories\": {\n        \"fruits\": [\"apple\", \"banana\"],\n        \"vegetables\": [\"carrot\", \"broccoli\"]\n    }\n}\n\nresult = (\n    etl(data)\n    .goto(\"categories\").each()  # Iterate categories (dict)\n    .each()                      # Iterate items in each category (list)\n    .map_to(table=\"items\", fields=[\n        Field(\"category\", parent_key()),  # \"fruits\" or \"vegetables\"\n        Field(\"name\", get([])),           # The item name\n        TempField(\"id\", get([]))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"items\"].values()), indent=2))\n\n[\n  {\n    \"category\": null,\n    \"name\": [\n      \"apple\",\n      \"banana\"\n    ],\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"category\": null,\n    \"name\": [\n      \"carrot\",\n      \"broccoli\"\n    ],\n    \"id\": \"__auto_1__\"\n  }\n]\n\n\n\n\nparent_index(depth=1) - Parent’s list index\nGets the list index from a parent iteration.\n\nfrom etielle import etl, Field, TempField, get, parent_index, index, concat, literal\nimport json\n\ndata = {\n    \"rows\": [\n        [\"a\", \"b\", \"c\"],\n        [\"d\", \"e\", \"f\"]\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"rows\").each()  # Iterate rows\n    .each()               # Iterate cells in each row\n    .map_to(table=\"cells\", fields=[\n        Field(\"row\", parent_index()),  # 0 or 1\n        Field(\"col\", index()),         # 0, 1, or 2\n        Field(\"value\", get([])),       # The cell value\n        TempField(\"id\", concat(literal(\"r\"), parent_index(), literal(\"c\"), index()))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"cells\"].values()), indent=2))\n\n[\n  {\n    \"row\": null,\n    \"col\": 0,\n    \"value\": [\n      \"a\",\n      \"b\",\n      \"c\"\n    ],\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"row\": null,\n    \"col\": 1,\n    \"value\": [\n      \"d\",\n      \"e\",\n      \"f\"\n    ],\n    \"id\": \"__auto_1__\"\n  }\n]\n\n\n\n\nnode() - Current node value\nReturns the entire current node. Useful when iterating over scalar values.\n\nfrom etielle import etl, Field, TempField, node\nimport json\n\ndata = {\"ids\": [101, 102, 103]}\n\nresult = (\n    etl(data)\n    .goto(\"ids\").each()\n    .map_to(table=\"items\", fields=[\n        Field(\"id\", node()),  # The id value itself\n        TempField(\"id\", node())\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"items\"].values()), indent=2))\n\n[\n  {\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"id\": \"__auto_2__\"\n  }\n]",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#combining-transforms",
    "href": "docs/transforms.html#combining-transforms",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "Combining Transforms",
    "text": "Combining Transforms\n\nconcat(*parts) - Join strings\nConcatenates multiple transforms into a single string.\n\nfrom etielle import etl, Field, TempField, get, literal, concat\nimport json\n\ndata = {\"users\": [{\"first\": \"Alice\", \"last\": \"Smith\", \"id\": \"u1\"}]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"full_name\", concat(get(\"first\"), literal(\" \"), get(\"last\"))),\n        Field(\"user_key\", concat(literal(\"user_\"), get(\"id\"))),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"users\"].values()), indent=2))\n\n[\n  {\n    \"full_name\": \"Alice Smith\",\n    \"user_key\": \"user_u1\",\n    \"id\": \"__auto_0__\"\n  }\n]\n\n\n\n\nformat_id(*parts, sep=\"_\") - Join with separator\nJoins non-empty parts with a separator. Skips None values.\n\nfrom etielle import etl, Field, TempField, get, format_id\nimport json\n\ndata = {\"records\": [{\"type\": \"post\", \"id\": \"123\", \"sub\": None}]}\n\nresult = (\n    etl(data)\n    .goto(\"records\").each()\n    .map_to(table=\"records\", fields=[\n        Field(\"key\", format_id(get(\"type\"), get(\"id\"), get(\"sub\"))),  # \"post_123\" (None skipped)\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"records\"].values()), indent=2))\n\n[\n  {\n    \"key\": \"post_123\",\n    \"id\": \"__auto_0__\"\n  }\n]\n\n\n\n\ncoalesce(*transforms) - First non-None value\nReturns the first non-None result from the given transforms.\n\nfrom etielle import etl, Field, TempField, get, literal, coalesce\nimport json\n\ndata = {\"users\": [\n    {\"id\": \"u1\", \"nickname\": \"Ali\", \"name\": \"Alice\"},\n    {\"id\": \"u2\", \"nickname\": None, \"name\": \"Bob\"}\n]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"display_name\", coalesce(get(\"nickname\"), get(\"name\"), literal(\"Anonymous\"))),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"users\"].values()), indent=2))\n\n[\n  {\n    \"display_name\": \"Ali\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"display_name\": \"Bob\",\n    \"id\": \"__auto_1__\"\n  }\n]\n\n\n\n\napply(func, inner) - Apply a function\nApplies any callable to the result of another transform. Useful for type coercion, string methods, and custom transformations.\n\nfrom etielle import etl, Field, TempField, get, apply\nimport json\n\ndata = {\"products\": [\n    {\"id\": \"p1\", \"name\": \"  Widget  \", \"price\": \"19.99\", \"quantity\": \"5\"}\n]}\n\nresult = (\n    etl(data)\n    .goto(\"products\").each()\n    .map_to(table=\"products\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", apply(str.strip, get(\"name\"))),       # String method\n        Field(\"price\", apply(float, get(\"price\"))),         # Type coercion\n        Field(\"quantity\", apply(int, get(\"quantity\"))),     # Type coercion\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"products\"].values()), indent=2))\n\n[\n  {\n    \"name\": \"Widget\",\n    \"price\": 19.99,\n    \"quantity\": 5,\n    \"id\": \"__auto_0__\"\n  }\n]\n\n\nYou can use any callable—built-in types, string methods, or custom functions:\napply(int, get(\"count\"))              # Convert to int\napply(float, get(\"price\"))            # Convert to float\napply(str.upper, get(\"code\"))         # Uppercase\napply(str.strip, get(\"name\"))         # Trim whitespace\napply(lambda x: x * 100, get(\"rate\")) # Custom transformation",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#transform-reference",
    "href": "docs/transforms.html#transform-reference",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "Transform Reference",
    "text": "Transform Reference\n\n\n\n\n\n\n\n\nTransform\nPurpose\nExample\n\n\n\n\nget(path)\nExtract from current node\nget(\"user.name\")\n\n\nget_from_parent(path, depth=1)\nExtract from ancestor\nget_from_parent(\"id\", depth=2)\n\n\nget_from_root(path)\nExtract from JSON root\nget_from_root(\"version\")\n\n\nliteral(value)\nConstant value\nliteral(\"active\")\n\n\nkey()\nCurrent dict key\nWhen iterating {\"a\": 1}\n\n\nindex()\nCurrent list index\n0, 1, 2, …\n\n\nparent_key(depth=1)\nParent’s dict key\nAccess parent iteration key\n\n\nparent_index(depth=1)\nParent’s list index\nAccess parent iteration index\n\n\nnode()\nCurrent node value\nThe entire current object\n\n\nconcat(*parts)\nJoin strings\nconcat(get(\"a\"), get(\"b\"))\n\n\nformat_id(*parts, sep=\"_\")\nJoin with separator\nformat_id(get(\"type\"), get(\"id\"))\n\n\ncoalesce(*transforms)\nFirst non-None\ncoalesce(get(\"nick\"), get(\"name\"))\n\n\napply(func, inner)\nApply function to result\napply(int, get(\"count\"))\n\n\nlen_of(inner)\nLength of list/dict/string\nlen_of(get(\"tags\"))",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#custom-transforms",
    "href": "docs/transforms.html#custom-transforms",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "Custom Transforms",
    "text": "Custom Transforms\nYou can create custom transforms using the @transform decorator. See Custom Transforms for details.",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/transforms.html#see-also",
    "href": "docs/transforms.html#see-also",
    "title": "Transforms: Extracting and Formatting Values",
    "section": "See also",
    "text": "See also\n\nNavigation - How to position yourself in the JSON\nMapping Tables - Using transforms in Field and TempField\nCustom Transforms - Creating your own transforms",
    "crumbs": [
      "Core Concepts",
      "Transforms"
    ]
  },
  {
    "objectID": "docs/relationships.html",
    "href": "docs/relationships.html",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "",
    "text": "What you’ll learn: How to use link_to() to build parent-child relationships between tables, avoiding N+1 database queries.\nETL context: Relationships are part of the Transform step—they link child records to parent records in memory before persistence.",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#what-is-link_to",
    "href": "docs/relationships.html#what-is-link_to",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "What is link_to()?",
    "text": "What is link_to()?\nlink_to() establishes a many-to-one relationship between the current table (child) and a parent table. It links records in memory so you can navigate between them or persist with proper foreign keys.\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[...])\n\n    .goto(\"posts\").each()\n    .map_to(table=Post, fields=[\n        Field(\"title\", get(\"title\")),\n        TempField(\"user_id\", get_from_parent(\"id\"))  # Foreign key value\n    ])\n    .link_to(User, by={\"user_id\": \"id\"})  # Link posts to users\n    .run()\n)",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#basic-relationship-binding",
    "href": "docs/relationships.html#basic-relationship-binding",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "Basic Relationship Binding",
    "text": "Basic Relationship Binding\n\nOne-to-Many (Users -&gt; Posts)\nThe most common pattern: users have many posts.\n\nfrom pydantic import BaseModel\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\nclass User(BaseModel):\n    id: str\n    name: str\n\nclass Post(BaseModel):\n    id: str\n    title: str\n    user: User | None = None  # Relationship attribute\n\ndata = {\n    \"users\": [{\n        \"id\": \"u1\",\n        \"name\": \"Alice\",\n        \"posts\": [\n            {\"id\": \"p1\", \"title\": \"Hello\"},\n            {\"id\": \"p2\", \"title\": \"World\"}\n        ]\n    }]\n}\n\nresult = (\n    etl(data)\n    # Map users\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n    ])\n\n    # Map posts and link to users\n    .goto(\"posts\").each()\n    .map_to(table=Post, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"title\", get(\"title\")),\n        TempField(\"user_id\", get_from_parent(\"id\"))  # Foreign key (not persisted)\n    ])\n    .link_to(User, by={\"user_id\": \"id\"})  # Link by user_id -&gt; id\n    .run()\n)\n\n# Posts now have user references\nfor key, post in result.tables[Post].items():\n    print(f\"{post.title} by {post.user.name if post.user else 'Unknown'}\")\n\nHello by Alice\nWorld by Alice",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#how-link_to-works",
    "href": "docs/relationships.html#how-link_to-works",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "How link_to() Works",
    "text": "How link_to() Works\n\nThe by Parameter\nThe by dict maps child field names to parent field names:\n.link_to(User, by={\"user_id\": \"id\"})\nThis means: “Match the child’s user_id TempField to the parent’s id TempField.”\n\n\nAttribute Inference\netielle infers the relationship attribute name from the parent table name:\n\n\n\nParent Table\nInferred Attribute\n\n\n\n\nusers\nuser\n\n\nposts\npost\n\n\ncategories\ncategorie (strips trailing ‘s’)\n\n\n\nYou can override this by using a model class with an explicit attribute.",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#multiple-relationships",
    "href": "docs/relationships.html#multiple-relationships",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "Multiple Relationships",
    "text": "Multiple Relationships\nA child table can link to multiple parent tables:\n\nfrom pydantic import BaseModel\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\nclass User(BaseModel):\n    id: str\n    name: str\n\nclass Post(BaseModel):\n    id: str\n    title: str\n\nclass Comment(BaseModel):\n    id: str\n    body: str\n    user: User | None = None\n    post: Post | None = None\n\ndata = {\n    \"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}],\n    \"posts\": [{\"id\": \"p1\", \"title\": \"Hello\", \"user_id\": \"u1\"}],\n    \"comments\": [\n        {\"id\": \"c1\", \"body\": \"Great post!\", \"user_id\": \"u1\", \"post_id\": \"p1\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n    ])\n\n    .goto_root()\n    .goto(\"posts\").each()\n    .map_to(table=Post, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"title\", get(\"title\")),\n    ])\n\n    .goto_root()\n    .goto(\"comments\").each()\n    .map_to(table=Comment, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"body\", get(\"body\")),\n        TempField(\"user_id\", get(\"user_id\")),  # Foreign key (not persisted)\n        TempField(\"post_id\", get(\"post_id\"))   # Foreign key (not persisted)\n    ])\n    .link_to(User, by={\"user_id\": \"id\"})  # Link to user\n    .link_to(Post, by={\"post_id\": \"id\"})  # Link to post\n    .run()\n)\n\ncomment = list(result.tables[Comment].values())[0]\nprint(f\"Comment: {comment.body}\")\nprint(f\"By: {comment.user.name if comment.user else 'Unknown'}\")\nprint(f\"On: {comment.post.title if comment.post else 'Unknown'}\")\n\nComment: Great post!\nBy: Alice\nOn: Hello",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#nested-data-relationships",
    "href": "docs/relationships.html#nested-data-relationships",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "Nested Data Relationships",
    "text": "Nested Data Relationships\nWhen data is nested, use get_from_parent() to extract the foreign key:\n\nfrom pydantic import BaseModel\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\nclass Order(BaseModel):\n    id: str\n    customer: str\n\nclass Item(BaseModel):\n    id: str\n    product: str\n    order: Order | None = None\n\ndata = {\n    \"orders\": [{\n        \"id\": \"o1\",\n        \"customer\": \"Alice\",\n        \"items\": [\n            {\"id\": \"i1\", \"product\": \"Widget\"},\n            {\"id\": \"i2\", \"product\": \"Gadget\"}\n        ]\n    }]\n}\n\nresult = (\n    etl(data)\n    .goto(\"orders\").each()\n    .map_to(table=Order, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"customer\", get(\"customer\")),\n    ])\n\n    .goto(\"items\").each()\n    .map_to(table=Item, fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"product\", get(\"product\")),\n        TempField(\"order_id\", get_from_parent(\"id\"))  # Foreign key (not persisted)\n    ])\n    .link_to(Order, by={\"order_id\": \"id\"})\n    .run()\n)\n\nfor key, item in result.tables[Item].items():\n    print(f\"{item.product} - Order: {item.order.id if item.order else 'None'}\")\n\nWidget - Order: o1\nGadget - Order: o1",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#composite-keys",
    "href": "docs/relationships.html#composite-keys",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "Composite Keys",
    "text": "Composite Keys\nFor relationships with composite keys, include multiple entries in the by dict:\n# Child has both region_id and store_id pointing to parent\n.link_to(Store, by={\n    \"region_id\": \"region_id\",\n    \"store_id\": \"store_id\"\n})",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#junction-tables-reverse-lookups",
    "href": "docs/relationships.html#junction-tables-reverse-lookups",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "Junction Tables (Reverse Lookups)",
    "text": "Junction Tables (Reverse Lookups)\nHandle junction tables where the relationship is defined outside the child:\n\n# Data: {\"userPosts\": {\"u1\": [101, 102], \"u2\": [103]}}\n# This maps user_id -&gt; list of post_ids\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[...])\n\n    .goto_root()\n    .goto(\"posts\").each()\n    .map_to(table=Post, fields=[...])\n\n    .goto_root()\n    .goto(\"userPosts\").each()    # key() = user_id\n    .each()                       # node() = post_id\n    .map_to(table=Post, join_on=[\"id\"], fields=[\n        TempField(\"id\", node()),\n        TempField(\"user_id\", parent_key())\n    ])\n    .link_to(User, by={\"user_id\": \"id\"})\n    .run()\n)",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#link_to-reference",
    "href": "docs/relationships.html#link_to-reference",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "link_to() Reference",
    "text": "link_to() Reference\n\n\n\nParameter\nType\nDescription\n\n\n\n\nparent\ntype\nThe parent model class\n\n\nby\ndict[str, str]\nMapping of {child_field: parent_field}\n\n\n\n\nRequirements\n\nMust be called after map_to() (links the most recent emission)\nBoth child and parent fields can be Field or TempField names\nParent table must be emitted somewhere in the pipeline\n\n\n\nError Handling\n\nMissing parents: By default, link_to() does not fail if a parent is not found\nThe relationship attribute will be None for unmatched children\nUse error handling modes to change this behavior",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#best-practices",
    "href": "docs/relationships.html#best-practices",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse TempField for Foreign Keys\nForeign key values should be in TempField, not Field, unless you also need them in output:\n# Good: Foreign key is TempField (not in output, just for linking)\nTempField(\"user_id\", get_from_parent(\"id\"))\n\n# If you need user_id in output: use Field (it's also available for linking)\nField(\"user_id\", get_from_parent(\"id\"))\n\n# Note: Don't use both Field and TempField with the same name\n# The TempField will shadow the Field and prevent it from being persisted\n\n\nOrder Matters\nEmit parent tables before child tables that link to them:\n# Good: Users emitted before posts\n.map_to(table=User, fields=[...])\n.goto(\"posts\").each()\n.map_to(table=Post, fields=[...])\n.link_to(User, by={\"user_id\": \"id\"})\n\n# Also works: Order within same run() is handled correctly\n\n\nMatch Key Types\nEnsure child and parent key values are the same type:\n# If parent key is string \"u1\"\nTempField(\"id\", get(\"id\"))  # Returns \"u1\"\n\n# Child key must also be string\nTempField(\"user_id\", get(\"user_id\"))  # Must return \"u1\", not 1",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "docs/relationships.html#see-also",
    "href": "docs/relationships.html#see-also",
    "title": "Relationships: Linking Tables with link_to()",
    "section": "See also",
    "text": "See also\n\nMapping Tables - Defining the tables that relationships link\nDatabase Loading - Persisting linked records to database\nNavigation - Nested traversal for parent-child data",
    "crumbs": [
      "Core Concepts",
      "Relationships"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "",
    "text": "etielle is a simple, powerful Python library for reshaping nested JSON data, typically from an API, into relational tables that fit your database schema. Think of etielle as a “JSON extractor” that you program with clear instructions: “Go here in the JSON, pull this data, and put it in that table.” The library’s name is a play on ETL (“Extract, Transform, Load”), which is the technical term for this set of operations.",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#why-use-etielle-for-beginners",
    "href": "index.html#why-use-etielle-for-beginners",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Why Use etielle? (For Beginners)",
    "text": "Why Use etielle? (For Beginners)\nJSON data from APIs is often deeply nested and requires complicated parsing. etielle helps by:\n\nTraversing nested structures: Walk through arrays-within-dictionaries-within-arrays to any arbitrary depth.\nPerforming arbitrary transformations: Use built-in functions or define your own custom transforms.\nBuilding relationships: Link records across your different output tables automatically.\nEmitting to arbitrary formats: Emit data to Pydantic models, TypedDicts, or ORM objects directly.\nOptionally loading data into a database: Load data into a database using SQLAlchemy or SQLModel.",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#learning-path",
    "href": "index.html#learning-path",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Learning Path",
    "text": "Learning Path\n\nQuickstart: Quick and dirty introduction to etielle and how to use it.\nIntroduction to ETL: The problem etielle is solving: JSON data ETL.\nNavigation: How to navigate through your JSON data with goto() and each().\nTransforms: Getting and altering values from the JSON data.\nMapping Tables: Outputting data with Field and TempField.\nRelationships: Linking tables together with link_to().\nDatabase Loading: Persisting data with load() and run().",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Installation",
    "text": "Installation\nWe recommend using uv for faster installs, but pip works too.\nuv add etielle\n# or\npip install etielle\n\nOptional: Install with ORM adapters\nIf you plan to bind relationships and flush to your database via SQLAlchemy or SQLModel, install with the optional extra for your ORM:\nuv add \"etielle[sqlalchemy]\"\n# or\nuv add \"etielle[sqlmodel]\"",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#quick-start-your-first-mapping",
    "href": "index.html#quick-start-your-first-mapping",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Quick Start: Your First Mapping",
    "text": "Quick Start: Your First Mapping\nLet’s start with a simple example. Suppose you have this JSON:\n\nimport json\n\ndata = {\n  \"users\": [\n    {\"id\": \"u1\", \"name\": \"Alice\", \"posts\": [{\"id\": \"p1\", \"title\": \"Hello\"}, {\"id\": \"p2\", \"title\": \"World\"}]},\n    {\"id\": \"u2\", \"name\": \"Bob\", \"posts\": []}\n  ]\n}\n\nWe want two tables: “users” (id, name) and “posts” (id, user_id, title).\nHere’s the code using the fluent API:\n\nfrom etielle import etl, Field, TempField, get, get_from_parent\n\n# Build and run the pipeline\nresult = (\n    etl(data)\n    # Extract users\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n    ])\n    # Extract posts (nested under each user)\n    .goto(\"posts\").each()\n    .map_to(table=\"posts\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"user_id\", get_from_parent(\"id\")),  # Link to parent user\n        Field(\"title\", get(\"title\")),\n    ])\n    .run()\n)\n\n# result.tables gives you dict access by table name\nout = {table: list(rows.values()) for table, rows in result.tables.items()}\nprint(json.dumps(out, indent=2))\n\n{\n  \"users\": [\n    {\n      \"id\": \"u1\",\n      \"name\": \"Alice\"\n    },\n    {\n      \"id\": \"u2\",\n      \"name\": \"Bob\"\n    }\n  ],\n  \"posts\": [\n    {\n      \"id\": \"p1\",\n      \"user_id\": \"u1\",\n      \"title\": \"Hello\"\n    },\n    {\n      \"id\": \"p2\",\n      \"user_id\": \"u1\",\n      \"title\": \"World\"\n    }\n  ]\n}\n\n\nCongrats! You’ve mapped your first JSON using the fluent E-&gt;T-&gt;L pattern.",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#core-concepts-breaking-it-down",
    "href": "index.html#core-concepts-breaking-it-down",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Core Concepts: Breaking It Down",
    "text": "Core Concepts: Breaking It Down\n\n1. The etl() Entry Point\nEverything starts with etl(data). This creates a pipeline builder that you chain methods onto:\nfrom etielle import etl\n\nresult = (\n    etl(data)           # Start with your JSON\n    .goto(\"users\")      # Navigate to a path\n    .each()             # Iterate over items\n    .map_to(...)        # Emit table rows\n    .run()              # Execute and get results\n)\n\n\n2. Navigation: goto() and each()\nNavigation tells etielle where to find your data:\n\ngoto(path): Navigate to a nested location (supports dot notation: \"data.users\")\neach(): Iterate over items in a list or dict\n\n# Navigate to data[\"response\"][\"users\"] and iterate\n.goto(\"response.users\").each()\n\n# Or use list syntax\n.goto([\"response\", \"users\"]).each()\n\n\n3. Fields and TempFields\nWhen you call map_to(), you define what data to extract:\n\nField(name, transform): A column that appears in your output\nTempField(name, transform): Used for joins/relationships, but NOT in output\n\n.map_to(table=\"users\", fields=[\n    Field(\"id\", get(\"id\")),          # Output column (also available for joins)\n    Field(\"name\", get(\"name\")),      # Output column\n])\n\n\n4. Transforms: Smart Data Extractors\nTransforms are functions that pull values from the current context:\n\n\n\nTransform\nPurpose\n\n\n\n\nget(\"name\")\nGet field from current node\n\n\nget_from_parent(\"id\")\nGet field from parent context\n\n\nget_from_root(\"config.version\")\nGet field from JSON root\n\n\nliteral(42)\nConstant value\n\n\nkey()\nCurrent dict key\n\n\nindex()\nCurrent list index\n\n\nconcat(a, b, c)\nJoin strings\n\n\n\n# Combine transforms\nField(\"full_id\", concat(literal(\"user_\"), get(\"id\")))  # \"user_u1\"\n\n\n5. Running the Pipeline\nCall .run() to execute and get results:\nresult = pipeline.run()\n\n# Access tables by name\nusers = result.tables[\"users\"]    # Dict[tuple, dict]\n\n# Or by model class (if using typed models)\nusers = result.tables[User]       # Dict[tuple, User]\n\n# Check for errors\nif result.errors:\n    for table, errs in result.errors.items():\n        print(f\"{table}: {errs}\")",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#detailed-examples",
    "href": "index.html#detailed-examples",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Detailed Examples",
    "text": "Detailed Examples\n\nExample 1: Merging Data from Multiple Paths\nMerge user info from two parts of JSON using goto_root():\n\nfrom etielle import etl, Field, TempField, get\n\ndata = {\n    \"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}],\n    \"profiles\": [{\"user_id\": \"u1\", \"email\": \"alice@example.com\"}]\n}\n\nresult = (\n    etl(data)\n    # First path: basic user data\n    .goto(\"users\").each()\n    .map_to(table=\"users\", join_on=[\"id\"], fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n    ])\n    # Second path: profile data (same table, merged by id)\n    .goto_root()\n    .goto(\"profiles\").each()\n    .map_to(table=\"users\", join_on=[\"id\"], fields=[\n        Field(\"email\", get(\"email\")),\n        TempField(\"id\", get(\"user_id\"))  # Join key for merging\n    ])\n    .run()\n)\n\nuser = list(result.tables[\"users\"].values())[0]\nprint(user)  # Has id, name, AND email merged together\n\n{'id': 'u1', 'name': 'Alice', 'email': 'alice@example.com'}\n\n\n\n\nExample 2: Deep Nesting\nHandle deeply nested structures with chained navigation:\n\n# servers -&gt; channels -&gt; messages -&gt; reactions (3 levels deep)\nresult = (\n    etl(data)\n    .goto(\"servers\").each()\n    .map_to(table=\"servers\", fields=[...])\n\n    .goto(\"channels\").each()\n    .map_to(table=\"channels\", fields=[\n        Field(\"server_id\", get_from_parent(\"id\", depth=1)),\n        ...\n    ])\n\n    .goto(\"messages\").each()\n    .map_to(table=\"messages\", fields=[\n        Field(\"channel_id\", get_from_parent(\"id\", depth=1)),\n        Field(\"server_id\", get_from_parent(\"id\", depth=2)),\n        ...\n    ])\n    .run()\n)\n\n\n\nExample 3: Typed Output with Pydantic\nUse model classes for validated, typed output:\n\nfrom pydantic import BaseModel\nfrom etielle import etl, Field, TempField, get\n\nclass User(BaseModel):\n    id: str\n    name: str\n    email: str | None = None\n\ndata = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[  # Pass model class, not string\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n    ])\n    .run()\n)\n\nuser = list(result.tables[User].values())[0]\nprint(f\"Type: {type(user).__name__}, name: {user.name}\")\n\nType: User, name: Alice",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#transform-cheatsheet",
    "href": "index.html#transform-cheatsheet",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Transform Cheatsheet",
    "text": "Transform Cheatsheet\n\n\n\n\n\n\n\n\nTransform\nPurpose\nExample\n\n\n\n\nget(path)\nFrom current node\nget(\"user.name\")\n\n\nget_from_parent(path, depth=1)\nFrom ancestor\nget_from_parent(\"id\")\n\n\nget_from_root(path)\nFrom JSON root\nget_from_root(\"version\")\n\n\nkey()\nCurrent dict key\nWhen iterating {\"a\": 1, \"b\": 2}\n\n\nindex()\nCurrent list index\n0, 1, 2, …\n\n\nparent_key()\nParent’s dict key\nAccess parent iteration key\n\n\nparent_index()\nParent’s list index\nAccess parent iteration index\n\n\nnode()\nCurrent node value\nThe whole current object\n\n\nliteral(value)\nConstant\nliteral(42)\n\n\nconcat(*parts)\nJoin strings\nconcat(get(\"first\"), literal(\" \"), get(\"last\"))\n\n\ncoalesce(*transforms)\nFirst non-None\ncoalesce(get(\"nickname\"), get(\"name\"))\n\n\nformat_id(*parts, sep=\"_\")\nJoin with separator\nformat_id(get(\"type\"), get(\"id\"))\n\n\nlen_of(inner)\nLength of list/dict/string\nlen_of(get(\"tags\"))",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#common-mistakes",
    "href": "index.html#common-mistakes",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\nEmpty results?\n\nCheck your goto() path matches the JSON structure exactly\nMake sure you called .each() to iterate\n\nMissing parent data?\n\nCheck the depth parameter in get_from_parent()\nEnsure the parent context exists in your navigation chain\n\nDuplicate or missing rows?\n\nVerify TempField values are unique for each row\nCheck that join keys don’t contain None values",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Next Steps",
    "text": "Next Steps\n\nNavigation - Deep dive into goto(), each(), and goto_root()\nTransforms - All built-in transforms and how to use them\nMapping Tables - Field, TempField, merge policies\nRelationships - Link tables with link_to()\nDatabase Loading - Persist with load().run()",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#glossary",
    "href": "index.html#glossary",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "Glossary",
    "text": "Glossary\n\nPipeline: The chain of operations from etl() to run()\nNavigation: Methods like goto() and each() that position you in the JSON\nTransform: A function that extracts values from the current context\nField: An output column in your table\nTempField: A field used for joins/linking but not in final output\nJoin Key: Values that uniquely identify a row (derived from TempFields)",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Quickstart: Declarative JSON-to-Relational Mapping in Python",
    "section": "License",
    "text": "License\nMIT\nNeed help? Open an issue on GitHub!",
    "crumbs": [
      "Getting Started",
      "Quickstart"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "Remove legacy SQLAlchemy/SQLModel adapters (33fd86c)\n\n\n\n\n\nAdd comprehensive test coverage for documented behaviors (1b5e138)\n\n\n\n\n\nThe legacy bind_and_flush adapters have been removed. Use the fluent API (.load(session).run()) instead.\n\n\n\n\n\n\n\n\nAdd supabase/ to .gitignore (15a76fc)\n\n\n\n\n\nAdd CONTRIBUTING.md with dev setup and Supabase test instructions (3d6525a)\n\n\n\n\n\nadapter: Add Supabase adapter (8c62339)\n\n\n\n\n\n\n\n\nAdd apply transform to documentation (487db7a)\n\n\n\n\n\nAdd apply transform for type coercion and function application (136badf)\n\n\n\n\n\n\n\n\nJoin_on fields are now persisted instead of excluded (6b7b895)\nProper flush ordering for NOT NULL FK constraints (ba23476)\nRemove fields() proxy and undeprecate field_of (6eed551)\nResolve spurious iteration bug (50af234)\nResolve type errors (eb30b0f)\nSingleton mapping without explicit join key now persists correctly (965ffd2)\nSingleton parents can now be linked by children (cb19d14)\n\n\n\n\n\nAdd fluent API documentation to README (ae9078a)\nFull documentation rewrite (c2b553c)\nUpdate docs to reflect architecture changes (b3744e7)\n\n\n\n\n\nAdd _build_dependency_graph to PipelineBuilder (682eef7)\nAdd _get_linkable_fields to extract linking fields from pipeline (f5e02d4)\nAdd bind_many_to_one_via_index for secondary index lookup (4ed9ab1)\nAdd indices field to MappingResult for secondary indices (cbd05c7)\nAdd topological_sort utility for dependency ordering (42b416e)\nBuild secondary indices for linkable fields during instance creation (4cdf311)\nDecouple join keys from relationship linking (4295408)\nExport fluent API from package root (4b3b4aa)\nImplement dependency-ordered flushing for auto-generated PKs (06864f8)\nfluent: Add @transform decorator for custom transforms (fada2b7)\nfluent: Add automatic model type detection for builders (a64ff0d)\nfluent: Add database persistence with load().run() (e14cd8c)\nfluent: Add each() iteration marker (8a817a0)\nfluent: Add Field dataclass for persisted fields (9e382c1)\nfluent: Add FieldUnion type alias (9f5ae39)\nfluent: Add goto() navigation method (9b3e14a)\nfluent: Add goto_root() navigation method (4c52b32)\nfluent: Add link_to() relationship method (bc67613)\nfluent: Add load() session configuration method (0f6edf8)\nfluent: Add map_to() emission method (09b1e6a)\nfluent: Add multiple root support in run() (3fc47ee)\nfluent: Add node() transform (5a5d5e1)\nfluent: Add parent_index() transform (0620c99)\nfluent: Add PipelineBuilder skeleton and etl() entry point (e5d1c0d)\nfluent: Add PipelineResult with _TablesProxy (2ed4844)\nfluent: Add relationship binding in run() (da4f6a2)\nfluent: Add run() execution method with basic extraction (dd8a627)\nfluent: Add TempField dataclass for join-only fields (5cd86b8)\n\n\n\n\n\nfluent: Add error handling mode tests (4b04968)\nfluent: Add tests for row merging with join_on (3e0382b)\n\n\n\n\n\n\n\n\nTransform from Protocol to TypeAlias (3f8a377)\n\n\n\n\n\n\n\n\nExport types (e8d66d9)\n\n\n\n\n\n\n\n\nMore logical sequence, better examples (16f9b32)\nResolve some link resolution issues (0663cbd)\n\n\n\n\n\nNew fields selector for class instances (0d0dc11)\n\n\n\n\n\n\n\n\nSupport &gt;1 spec per child table (57ee01e)\n\n\n\n\n\nLint code (9f4d3c8)\n\n\n\n\n\nAdd comprehensive introduction to ETL documentation (69e8029)\nIntroduction to ETL (90e0fd1)\nRefine ETL concept mapping in introduction (bbdb9a7)\n\n\n\n\n\n\n\n\nRemove lockfile from version control (0229547)\n\n\n\n\n\nNew ConstructorBuilder for ORMs (977b0a0)\n\n\n\n\n\n\n\n\nClarify field selector docs (cb90959)\nCross-linking, user-friendliness (acb8b87)\nImprove clarity of README (dbc75c2)\nMore detailed emission guide (782ab62)\n\n\n\n\n\nConsolidate/enhance adapter documentation (e6df5ed)\n\n\n\n\n\n\n\n\nMake release workflow ff after publish (506d3c8)\n\n\n\n\n\nAdd backticks around code (e251f3f)\nAll documentation code runs (61dbb24)\nDocumentation website (8019c14)\nExample code triggers errors (b12093e)\nFix header text color (69ab9d4)\nPrint mapping result (1ef1164)\n\n\n\n\n\nSimplify iteration API (25e1c17)\n\n\n\n\n\n\n\n\nSqlalchemy adapter (3ac07a1)\n\n\n\n\n\n\n\n\nMutation-based emit (21dbbe6)\n\n\n\n\n\n\n\n\nError reporting (10657ec)\nInstance emission adapter (4c02de0)\n\n\n\n\n\n\n\n\nImplement field selectors API (e099dbe)\n\n\n\n\n\n\n\n\nConfigure pyproject version stamping (49c4e07)\n\n\n\n\n\n\n\n\nMake release dependent on test (b377958)\n\n\n\n\n\n\n\n\nCorrectly use PyPi env for publish job (2dd6cea)\n\n\n\n\n\n\n\n\nRun build in PSR container (6c359c6)\n\n\n\n\n\n\n\n\nRestore build step (4250642)\n\n\n\n\n\n\n\n\nGate artifact upload (5e5bda1)\n\n\n\n\n\n\n\n\nActions-compliant root path (251d553)\nAdd missing semantic release config (f6235d4)\nSemantic release version mismatch (3dbf9a0)\nUse semantic release’s built-in committer (6681c31)"
  },
  {
    "objectID": "CHANGELOG.html#v3.0.0-2025-12-02",
    "href": "CHANGELOG.html#v3.0.0-2025-12-02",
    "title": "Changelog",
    "section": "",
    "text": "Remove legacy SQLAlchemy/SQLModel adapters (33fd86c)\n\n\n\n\n\nAdd comprehensive test coverage for documented behaviors (1b5e138)\n\n\n\n\n\nThe legacy bind_and_flush adapters have been removed. Use the fluent API (.load(session).run()) instead."
  },
  {
    "objectID": "CHANGELOG.html#v2.6.0-2025-12-02",
    "href": "CHANGELOG.html#v2.6.0-2025-12-02",
    "title": "Changelog",
    "section": "",
    "text": "Add supabase/ to .gitignore (15a76fc)\n\n\n\n\n\nAdd CONTRIBUTING.md with dev setup and Supabase test instructions (3d6525a)\n\n\n\n\n\nadapter: Add Supabase adapter (8c62339)"
  },
  {
    "objectID": "CHANGELOG.html#v2.5.0-2025-12-01",
    "href": "CHANGELOG.html#v2.5.0-2025-12-01",
    "title": "Changelog",
    "section": "",
    "text": "Add apply transform to documentation (487db7a)\n\n\n\n\n\nAdd apply transform for type coercion and function application (136badf)"
  },
  {
    "objectID": "CHANGELOG.html#v2.4.0-2025-12-01",
    "href": "CHANGELOG.html#v2.4.0-2025-12-01",
    "title": "Changelog",
    "section": "",
    "text": "Join_on fields are now persisted instead of excluded (6b7b895)\nProper flush ordering for NOT NULL FK constraints (ba23476)\nRemove fields() proxy and undeprecate field_of (6eed551)\nResolve spurious iteration bug (50af234)\nResolve type errors (eb30b0f)\nSingleton mapping without explicit join key now persists correctly (965ffd2)\nSingleton parents can now be linked by children (cb19d14)\n\n\n\n\n\nAdd fluent API documentation to README (ae9078a)\nFull documentation rewrite (c2b553c)\nUpdate docs to reflect architecture changes (b3744e7)\n\n\n\n\n\nAdd _build_dependency_graph to PipelineBuilder (682eef7)\nAdd _get_linkable_fields to extract linking fields from pipeline (f5e02d4)\nAdd bind_many_to_one_via_index for secondary index lookup (4ed9ab1)\nAdd indices field to MappingResult for secondary indices (cbd05c7)\nAdd topological_sort utility for dependency ordering (42b416e)\nBuild secondary indices for linkable fields during instance creation (4cdf311)\nDecouple join keys from relationship linking (4295408)\nExport fluent API from package root (4b3b4aa)\nImplement dependency-ordered flushing for auto-generated PKs (06864f8)\nfluent: Add @transform decorator for custom transforms (fada2b7)\nfluent: Add automatic model type detection for builders (a64ff0d)\nfluent: Add database persistence with load().run() (e14cd8c)\nfluent: Add each() iteration marker (8a817a0)\nfluent: Add Field dataclass for persisted fields (9e382c1)\nfluent: Add FieldUnion type alias (9f5ae39)\nfluent: Add goto() navigation method (9b3e14a)\nfluent: Add goto_root() navigation method (4c52b32)\nfluent: Add link_to() relationship method (bc67613)\nfluent: Add load() session configuration method (0f6edf8)\nfluent: Add map_to() emission method (09b1e6a)\nfluent: Add multiple root support in run() (3fc47ee)\nfluent: Add node() transform (5a5d5e1)\nfluent: Add parent_index() transform (0620c99)\nfluent: Add PipelineBuilder skeleton and etl() entry point (e5d1c0d)\nfluent: Add PipelineResult with _TablesProxy (2ed4844)\nfluent: Add relationship binding in run() (da4f6a2)\nfluent: Add run() execution method with basic extraction (dd8a627)\nfluent: Add TempField dataclass for join-only fields (5cd86b8)\n\n\n\n\n\nfluent: Add error handling mode tests (4b04968)\nfluent: Add tests for row merging with join_on (3e0382b)"
  },
  {
    "objectID": "CHANGELOG.html#v2.3.2-2025-11-29",
    "href": "CHANGELOG.html#v2.3.2-2025-11-29",
    "title": "Changelog",
    "section": "",
    "text": "Transform from Protocol to TypeAlias (3f8a377)"
  },
  {
    "objectID": "CHANGELOG.html#v2.3.1-2025-11-29",
    "href": "CHANGELOG.html#v2.3.1-2025-11-29",
    "title": "Changelog",
    "section": "",
    "text": "Export types (e8d66d9)"
  },
  {
    "objectID": "CHANGELOG.html#v2.3.0-2025-11-29",
    "href": "CHANGELOG.html#v2.3.0-2025-11-29",
    "title": "Changelog",
    "section": "",
    "text": "More logical sequence, better examples (16f9b32)\nResolve some link resolution issues (0663cbd)\n\n\n\n\n\nNew fields selector for class instances (0d0dc11)"
  },
  {
    "objectID": "CHANGELOG.html#v2.2.1-2025-11-25",
    "href": "CHANGELOG.html#v2.2.1-2025-11-25",
    "title": "Changelog",
    "section": "",
    "text": "Support &gt;1 spec per child table (57ee01e)\n\n\n\n\n\nLint code (9f4d3c8)\n\n\n\n\n\nAdd comprehensive introduction to ETL documentation (69e8029)\nIntroduction to ETL (90e0fd1)\nRefine ETL concept mapping in introduction (bbdb9a7)"
  },
  {
    "objectID": "CHANGELOG.html#v2.2.0-2025-10-23",
    "href": "CHANGELOG.html#v2.2.0-2025-10-23",
    "title": "Changelog",
    "section": "",
    "text": "Remove lockfile from version control (0229547)\n\n\n\n\n\nNew ConstructorBuilder for ORMs (977b0a0)"
  },
  {
    "objectID": "CHANGELOG.html#v2.1.0-2025-10-20",
    "href": "CHANGELOG.html#v2.1.0-2025-10-20",
    "title": "Changelog",
    "section": "",
    "text": "Clarify field selector docs (cb90959)\nCross-linking, user-friendliness (acb8b87)\nImprove clarity of README (dbc75c2)\nMore detailed emission guide (782ab62)\n\n\n\n\n\nConsolidate/enhance adapter documentation (e6df5ed)"
  },
  {
    "objectID": "CHANGELOG.html#v2.0.0-2025-10-19",
    "href": "CHANGELOG.html#v2.0.0-2025-10-19",
    "title": "Changelog",
    "section": "",
    "text": "Make release workflow ff after publish (506d3c8)\n\n\n\n\n\nAdd backticks around code (e251f3f)\nAll documentation code runs (61dbb24)\nDocumentation website (8019c14)\nExample code triggers errors (b12093e)\nFix header text color (69ab9d4)\nPrint mapping result (1ef1164)\n\n\n\n\n\nSimplify iteration API (25e1c17)"
  },
  {
    "objectID": "CHANGELOG.html#v1.4.0-2025-10-15",
    "href": "CHANGELOG.html#v1.4.0-2025-10-15",
    "title": "Changelog",
    "section": "",
    "text": "Sqlalchemy adapter (3ac07a1)"
  },
  {
    "objectID": "CHANGELOG.html#v1.3.0-2025-10-14",
    "href": "CHANGELOG.html#v1.3.0-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Mutation-based emit (21dbbe6)"
  },
  {
    "objectID": "CHANGELOG.html#v1.2.0-2025-10-14",
    "href": "CHANGELOG.html#v1.2.0-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Error reporting (10657ec)\nInstance emission adapter (4c02de0)"
  },
  {
    "objectID": "CHANGELOG.html#v1.1.0-2025-10-14",
    "href": "CHANGELOG.html#v1.1.0-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Implement field selectors API (e099dbe)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.6-2025-10-14",
    "href": "CHANGELOG.html#v1.0.6-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Configure pyproject version stamping (49c4e07)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.5-2025-10-14",
    "href": "CHANGELOG.html#v1.0.5-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Make release dependent on test (b377958)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.4-2025-10-14",
    "href": "CHANGELOG.html#v1.0.4-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Correctly use PyPi env for publish job (2dd6cea)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.3-2025-10-14",
    "href": "CHANGELOG.html#v1.0.3-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Run build in PSR container (6c359c6)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.2-2025-10-14",
    "href": "CHANGELOG.html#v1.0.2-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Restore build step (4250642)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.1-2025-10-14",
    "href": "CHANGELOG.html#v1.0.1-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Gate artifact upload (5e5bda1)"
  },
  {
    "objectID": "CHANGELOG.html#v1.0.0-2025-10-14",
    "href": "CHANGELOG.html#v1.0.0-2025-10-14",
    "title": "Changelog",
    "section": "",
    "text": "Actions-compliant root path (251d553)\nAdd missing semantic release config (f6235d4)\nSemantic release version mismatch (3dbf9a0)\nUse semantic release’s built-in committer (6681c31)"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to etielle",
    "section": "",
    "text": "Python 3.13+\nuv for dependency management\nDocker (for Supabase integration tests)\n\n\n\n\n# Clone the repository\ngit clone https://github.com/Promptly-Technologies-LLC/etielle.git\ncd etielle\n\n# Install dependencies (including dev dependencies)\nuv sync\n\n\n\n# Run all tests\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_transforms.py -v\n\n# Run with coverage\nuv run pytest tests/ --cov=etielle\n\n\n\n# Run linter\nuv run ruff check etielle/\n\n# Run type checker\nuv run mypy etielle/\n\n\n\n\nThe Supabase adapter has integration tests that require a running Supabase instance. These tests are skipped by default when the required environment variables are not set.\n\n\n\nInstall Supabase CLI\n# macOS\nbrew install supabase/tap/supabase\n\n# Linux/macOS (via bun - preferred)\nbun install -g supabase\n\n# Or via npm\nnpm install -g supabase\n\n# Or download from https://github.com/supabase/cli/releases\nStart Local Supabase Stack\n# Initialize Supabase in the project (first time only)\nnpx supabase init\n\n# Start the local Supabase stack (runs ~10 Docker containers)\nnpx supabase start\nThis will output connection details including:\n\nAPI URL: http://127.0.0.1:54321\nPublishable key (e.g., sb_publishable_...)\nDatabase URL: postgresql://postgres:postgres@127.0.0.1:54322/postgres\n\nCreate Test Tables\nConnect to the local Supabase database and create the test tables:\nPGPASSWORD=postgres psql -h 127.0.0.1 -p 54322 -U postgres -d postgres -c \"\nCREATE TABLE IF NOT EXISTS test_users (\n    id TEXT PRIMARY KEY,\n    name TEXT NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS test_posts (\n    id TEXT PRIMARY KEY,\n    title TEXT NOT NULL,\n    user_id TEXT REFERENCES test_users(id)\n);\n\"\nOr use the Supabase Studio at http://127.0.0.1:54323.\nSet Environment Variables\nGet the publishable key from npx supabase status output:\nexport SUPABASE_URL=\"http://127.0.0.1:54321\"\nexport SUPABASE_KEY=\"sb_publishable_...\"  # from supabase status\n\n\n\n\n# Run only Supabase tests\nuv run pytest tests/test_supabase_adapter.py -v\n\n# Run all tests (Supabase tests will run if env vars are set)\nuv run pytest tests/ -v\nThe integration tests in TestSupabaseIntegration class will: - Clean up test tables before/after each test - Insert real data to Supabase - Verify data was persisted correctly\n\n\n\n# Stop the Supabase stack\nsupabase stop\n\n# Stop and remove all data\nsupabase stop --no-backup\n\n\n\nTests still skipped after setting env vars: - Ensure variables are exported in the current shell - Check echo $SUPABASE_URL returns the expected value\nConnection refused errors: - Verify Supabase is running: supabase status - Check Docker containers: docker ps | grep supabase\nTable does not exist errors: - Create the test tables as shown above - Verify via Supabase Studio at http://localhost:54323\n\n\n\n\n\nUse ruff for linting\nFollow existing patterns in the codebase\nAdd tests for new functionality (TDD preferred)\nUpdate documentation for user-facing changes\n\n\n\n\n\nCreate a feature branch from main\nMake your changes with tests\nEnsure all tests pass: uv run pytest tests/ -v\nEnsure linting passes: uv run ruff check etielle/\nUpdate documentation if needed\nSubmit PR with clear description of changes"
  },
  {
    "objectID": "CONTRIBUTING.html#development-environment-setup",
    "href": "CONTRIBUTING.html#development-environment-setup",
    "title": "Contributing to etielle",
    "section": "",
    "text": "Python 3.13+\nuv for dependency management\nDocker (for Supabase integration tests)\n\n\n\n\n# Clone the repository\ngit clone https://github.com/Promptly-Technologies-LLC/etielle.git\ncd etielle\n\n# Install dependencies (including dev dependencies)\nuv sync\n\n\n\n# Run all tests\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_transforms.py -v\n\n# Run with coverage\nuv run pytest tests/ --cov=etielle\n\n\n\n# Run linter\nuv run ruff check etielle/\n\n# Run type checker\nuv run mypy etielle/"
  },
  {
    "objectID": "CONTRIBUTING.html#supabase-integration-tests",
    "href": "CONTRIBUTING.html#supabase-integration-tests",
    "title": "Contributing to etielle",
    "section": "",
    "text": "The Supabase adapter has integration tests that require a running Supabase instance. These tests are skipped by default when the required environment variables are not set.\n\n\n\nInstall Supabase CLI\n# macOS\nbrew install supabase/tap/supabase\n\n# Linux/macOS (via bun - preferred)\nbun install -g supabase\n\n# Or via npm\nnpm install -g supabase\n\n# Or download from https://github.com/supabase/cli/releases\nStart Local Supabase Stack\n# Initialize Supabase in the project (first time only)\nnpx supabase init\n\n# Start the local Supabase stack (runs ~10 Docker containers)\nnpx supabase start\nThis will output connection details including:\n\nAPI URL: http://127.0.0.1:54321\nPublishable key (e.g., sb_publishable_...)\nDatabase URL: postgresql://postgres:postgres@127.0.0.1:54322/postgres\n\nCreate Test Tables\nConnect to the local Supabase database and create the test tables:\nPGPASSWORD=postgres psql -h 127.0.0.1 -p 54322 -U postgres -d postgres -c \"\nCREATE TABLE IF NOT EXISTS test_users (\n    id TEXT PRIMARY KEY,\n    name TEXT NOT NULL\n);\n\nCREATE TABLE IF NOT EXISTS test_posts (\n    id TEXT PRIMARY KEY,\n    title TEXT NOT NULL,\n    user_id TEXT REFERENCES test_users(id)\n);\n\"\nOr use the Supabase Studio at http://127.0.0.1:54323.\nSet Environment Variables\nGet the publishable key from npx supabase status output:\nexport SUPABASE_URL=\"http://127.0.0.1:54321\"\nexport SUPABASE_KEY=\"sb_publishable_...\"  # from supabase status\n\n\n\n\n# Run only Supabase tests\nuv run pytest tests/test_supabase_adapter.py -v\n\n# Run all tests (Supabase tests will run if env vars are set)\nuv run pytest tests/ -v\nThe integration tests in TestSupabaseIntegration class will: - Clean up test tables before/after each test - Insert real data to Supabase - Verify data was persisted correctly\n\n\n\n# Stop the Supabase stack\nsupabase stop\n\n# Stop and remove all data\nsupabase stop --no-backup\n\n\n\nTests still skipped after setting env vars: - Ensure variables are exported in the current shell - Check echo $SUPABASE_URL returns the expected value\nConnection refused errors: - Verify Supabase is running: supabase status - Check Docker containers: docker ps | grep supabase\nTable does not exist errors: - Create the test tables as shown above - Verify via Supabase Studio at http://localhost:54323"
  },
  {
    "objectID": "CONTRIBUTING.html#code-style",
    "href": "CONTRIBUTING.html#code-style",
    "title": "Contributing to etielle",
    "section": "",
    "text": "Use ruff for linting\nFollow existing patterns in the codebase\nAdd tests for new functionality (TDD preferred)\nUpdate documentation for user-facing changes"
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "Contributing to etielle",
    "section": "",
    "text": "Create a feature branch from main\nMake your changes with tests\nEnsure all tests pass: uv run pytest tests/ -v\nEnsure linting passes: uv run ruff check etielle/\nUpdate documentation if needed\nSubmit PR with clear description of changes"
  },
  {
    "objectID": "docs/custom-transforms.html",
    "href": "docs/custom-transforms.html",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "",
    "text": "What you’ll learn: How to create custom transforms using the @transform decorator for reusable data extraction logic.\nETL context: Custom transforms extend the Transform step with your own domain-specific data extraction functions.",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#what-is-transform",
    "href": "docs/custom-transforms.html#what-is-transform",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "What is @transform?",
    "text": "What is @transform?\nThe @transform decorator converts a function into a reusable transform factory. The function receives a Context object as its first parameter, and any additional parameters become factory arguments.\nfrom etielle import transform\nfrom etielle.core import Context\n\n@transform\ndef uppercase(ctx: Context, field: str) -&gt; str | None:\n    \"\"\"Extract a field and convert to uppercase.\"\"\"\n    value = ctx.node.get(field)\n    return value.upper() if value else None\n\n# Usage\nField(\"name\", uppercase(\"name\"))  # Returns \"ALICE\" for {\"name\": \"Alice\"}",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#basic-custom-transforms",
    "href": "docs/custom-transforms.html#basic-custom-transforms",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "Basic Custom Transforms",
    "text": "Basic Custom Transforms\n\nSimple Field Transformation\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport json\n\n@transform\ndef initials(ctx: Context, first_field: str, last_field: str) -&gt; str:\n    \"\"\"Extract initials from first and last name.\"\"\"\n    first = ctx.node.get(first_field, \"\")\n    last = ctx.node.get(last_field, \"\")\n    return f\"{first[0]}{last[0]}\" if first and last else \"\"\n\ndata = {\"users\": [\n    {\"id\": \"u1\", \"first_name\": \"Alice\", \"last_name\": \"Smith\"},\n    {\"id\": \"u2\", \"first_name\": \"Bob\", \"last_name\": \"Jones\"}\n]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"initials\", initials(\"first_name\", \"last_name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"users\"].values()), indent=2))\n\n[\n  {\n    \"initials\": \"AS\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"initials\": \"BJ\",\n    \"id\": \"__auto_1__\"\n  }\n]\n\n\n\n\nTransform with Default Arguments\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport json\n\n@transform\ndef truncate(ctx: Context, field: str, max_length: int = 50) -&gt; str | None:\n    \"\"\"Truncate a field to max_length characters.\"\"\"\n    value = ctx.node.get(field)\n    if value is None:\n        return None\n    return value[:max_length] + \"...\" if len(value) &gt; max_length else value\n\ndata = {\"posts\": [\n    {\"id\": \"p1\", \"title\": \"Short title\"},\n    {\"id\": \"p2\", \"title\": \"This is a very long title that should be truncated for display\"}\n]}\n\nresult = (\n    etl(data)\n    .goto(\"posts\").each()\n    .map_to(table=\"posts\", fields=[\n        Field(\"title\", truncate(\"title\", max_length=20)),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"posts\"].values()), indent=2))\n\n[\n  {\n    \"title\": \"Short title\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"title\": \"This is a very long ...\",\n    \"id\": \"__auto_1__\"\n  }\n]",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#using-context-fields",
    "href": "docs/custom-transforms.html#using-context-fields",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "Using Context Fields",
    "text": "Using Context Fields\nThe Context object provides access to:\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nctx.node\nAny\nCurrent JSON node being processed\n\n\nctx.root\nAny\nThe entire JSON payload\n\n\nctx.parent\nContext \\| None\nParent context (one level up)\n\n\nctx.key\nstr \\| None\nCurrent dict key (when iterating dict)\n\n\nctx.index\nint \\| None\nCurrent list index (when iterating list)\n\n\nctx.path\ntuple\nFull path from root to current node\n\n\n\n\nAccessing Parent Context\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport json\n\n@transform\ndef full_path(ctx: Context) -&gt; str:\n    \"\"\"Build a path string from parent name and current name.\"\"\"\n    parent_name = \"\"\n    if ctx.parent and ctx.parent.node:\n        parent_name = ctx.parent.node.get(\"name\", \"\")\n\n    current_name = ctx.node.get(\"name\", \"\")\n    return f\"{parent_name}/{current_name}\" if parent_name else current_name\n\ndata = {\n    \"categories\": [{\n        \"name\": \"Electronics\",\n        \"items\": [\n            {\"name\": \"Phone\"},\n            {\"name\": \"Laptop\"}\n        ]\n    }]\n}\n\nresult = (\n    etl(data)\n    .goto(\"categories\").each()\n    .goto(\"items\").each()\n    .map_to(table=\"items\", fields=[\n        Field(\"path\", full_path()),\n        TempField(\"name\", get(\"name\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"items\"].values()), indent=2))\n\n[\n  {\n    \"path\": \"Electronics/Phone\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"path\": \"Electronics/Laptop\",\n    \"id\": \"__auto_1__\"\n  }\n]\n\n\n\n\nUsing Root Context\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport json\n\n@transform\ndef with_prefix(ctx: Context, field: str) -&gt; str:\n    \"\"\"Prefix a field value with the root's prefix setting.\"\"\"\n    prefix = ctx.root.get(\"prefix\", \"\")\n    value = ctx.node.get(field, \"\")\n    return f\"{prefix}{value}\"\n\ndata = {\n    \"prefix\": \"APP_\",\n    \"settings\": [\n        {\"key\": \"debug\", \"value\": \"true\"},\n        {\"key\": \"timeout\", \"value\": \"30\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"settings\").each()\n    .map_to(table=\"settings\", fields=[\n        Field(\"key\", with_prefix(\"key\")),\n        Field(\"value\", get(\"value\")),\n        TempField(\"key\", get(\"key\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"settings\"].values()), indent=2))\n\n[\n  {\n    \"value\": \"true\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"value\": \"30\",\n    \"id\": \"__auto_1__\"\n  }\n]",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#transform-patterns",
    "href": "docs/custom-transforms.html#transform-patterns",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "Transform Patterns",
    "text": "Transform Patterns\n\nLookup Transform\nUse closures to create transforms that reference external data:\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport json\n\n# External lookup data\nCATEGORY_NAMES = {\n    \"cat1\": \"Electronics\",\n    \"cat2\": \"Clothing\",\n    \"cat3\": \"Books\"\n}\n\n@transform\ndef lookup_category(ctx: Context, field: str) -&gt; str | None:\n    \"\"\"Look up category name from ID.\"\"\"\n    category_id = ctx.node.get(field)\n    return CATEGORY_NAMES.get(category_id, \"Unknown\")\n\ndata = {\n    \"products\": [\n        {\"id\": \"p1\", \"name\": \"Phone\", \"category_id\": \"cat1\"},\n        {\"id\": \"p2\", \"name\": \"Shirt\", \"category_id\": \"cat2\"},\n        {\"id\": \"p3\", \"name\": \"Novel\", \"category_id\": \"cat3\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"products\").each()\n    .map_to(table=\"products\", fields=[\n        Field(\"name\", get(\"name\")),\n        Field(\"category\", lookup_category(\"category_id\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"products\"].values()), indent=2))\n\n[\n  {\n    \"name\": \"Phone\",\n    \"category\": \"Electronics\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"name\": \"Shirt\",\n    \"category\": \"Clothing\",\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"name\": \"Novel\",\n    \"category\": \"Books\",\n    \"id\": \"__auto_2__\"\n  }\n]\n\n\n\n\nValidation Transform\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport re\nimport json\n\n@transform\ndef validate_email(ctx: Context, field: str) -&gt; str | None:\n    \"\"\"Validate and normalize email address.\"\"\"\n    email = ctx.node.get(field)\n    if not email:\n        return None\n\n    email = email.strip().lower()\n    if re.match(r\"^[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}$\", email):\n        return email\n    return None  # Invalid email returns None\n\ndata = {\n    \"contacts\": [\n        {\"id\": \"c1\", \"email\": \"Alice@Example.COM\"},\n        {\"id\": \"c2\", \"email\": \"not-an-email\"},\n        {\"id\": \"c3\", \"email\": \"bob@test.org\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"contacts\").each()\n    .map_to(table=\"contacts\", fields=[\n        Field(\"email\", validate_email(\"email\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"contacts\"].values()), indent=2))\n\n[\n  {\n    \"email\": \"alice@example.com\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"email\": null,\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"email\": \"bob@test.org\",\n    \"id\": \"__auto_2__\"\n  }\n]\n\n\n\n\nComputed Key Transform\n\nfrom etielle import etl, Field, TempField, get, transform\nfrom etielle.core import Context\nimport hashlib\nimport json\n\n@transform\ndef hash_key(ctx: Context, *fields: str) -&gt; str:\n    \"\"\"Generate a hash key from multiple fields.\"\"\"\n    parts = [str(ctx.node.get(f, \"\")) for f in fields]\n    combined = \"|\".join(parts)\n    return hashlib.md5(combined.encode()).hexdigest()[:8]\n\ndata = {\n    \"events\": [\n        {\"date\": \"2024-01-15\", \"type\": \"login\", \"user\": \"alice\"},\n        {\"date\": \"2024-01-15\", \"type\": \"logout\", \"user\": \"alice\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"events\").each()\n    .map_to(table=\"events\", fields=[\n        Field(\"date\", get(\"date\")),\n        Field(\"type\", get(\"type\")),\n        Field(\"user\", get(\"user\")),\n        TempField(\"id\", hash_key(\"date\", \"type\", \"user\"))\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"events\"].values()), indent=2))\n\n[\n  {\n    \"date\": \"2024-01-15\",\n    \"type\": \"login\",\n    \"user\": \"alice\",\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"date\": \"2024-01-15\",\n    \"type\": \"logout\",\n    \"user\": \"alice\",\n    \"id\": \"__auto_1__\"\n  }\n]",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#no-argument-transforms",
    "href": "docs/custom-transforms.html#no-argument-transforms",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "No-Argument Transforms",
    "text": "No-Argument Transforms\nFor transforms that don’t need parameters, the decorator still works:\n\nfrom etielle import etl, Field, TempField, transform\nfrom etielle.core import Context\nimport json\n\n@transform\ndef current_path(ctx: Context) -&gt; str:\n    \"\"\"Return the JSON path to current node as a string.\"\"\"\n    return \".\".join(str(p) for p in ctx.path)\n\n@transform\ndef is_first(ctx: Context) -&gt; bool:\n    \"\"\"Check if this is the first item in an iteration.\"\"\"\n    return ctx.index == 0\n\ndata = {\"items\": [\"a\", \"b\", \"c\"]}\n\nresult = (\n    etl(data)\n    .goto(\"items\").each()\n    .map_to(table=\"items\", fields=[\n        Field(\"path\", current_path()),\n        Field(\"is_first\", is_first()),\n        TempField(\"index\", lambda ctx: ctx.index)\n    ])\n    .run()\n)\n\nprint(json.dumps(list(result.tables[\"items\"].values()), indent=2))\n\n[\n  {\n    \"path\": \"items.0\",\n    \"is_first\": true,\n    \"id\": \"__auto_0__\"\n  },\n  {\n    \"path\": \"items.1\",\n    \"is_first\": false,\n    \"id\": \"__auto_1__\"\n  },\n  {\n    \"path\": \"items.2\",\n    \"is_first\": false,\n    \"id\": \"__auto_2__\"\n  }\n]",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#best-practices",
    "href": "docs/custom-transforms.html#best-practices",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "Best Practices",
    "text": "Best Practices\n\nKeep Transforms Pure\nTransforms should be side-effect free:\n# Good: Pure function\n@transform\ndef uppercase(ctx: Context, field: str) -&gt; str | None:\n    value = ctx.node.get(field)\n    return value.upper() if value else None\n\n# Bad: Side effects\ncounter = 0\n\n@transform\ndef counted(ctx: Context) -&gt; int:\n    global counter\n    counter += 1  # Side effect!\n    return counter\n\n\nHandle Missing Data\nReturn None rather than raising exceptions:\n# Good: Defensive\n@transform\ndef safe_get(ctx: Context, field: str) -&gt; str | None:\n    try:\n        return ctx.node[field]\n    except (KeyError, TypeError):\n        return None\n\n# Bad: May raise\n@transform\ndef unsafe_get(ctx: Context, field: str) -&gt; str:\n    return ctx.node[field]  # Raises if missing!\n\n\nDocument Complex Logic\n@transform\ndef compute_score(ctx: Context) -&gt; float:\n    \"\"\"\n    Calculate weighted score for a product.\n\n    Formula: (rating * 0.7) + (review_count_normalized * 0.3)\n    - rating: 1-5 star rating\n    - review_count_normalized: log10(review_count + 1) / 5\n\n    Returns 0.0 if rating is missing.\n    \"\"\"\n    rating = ctx.node.get(\"rating\", 0)\n    reviews = ctx.node.get(\"review_count\", 0)\n\n    import math\n    normalized_reviews = math.log10(reviews + 1) / 5\n    return round(rating * 0.7 + normalized_reviews * 0.3, 2)\n\n\nType Hints\nUse type hints for better IDE support:\n@transform\ndef parse_date(ctx: Context, field: str, format: str = \"%Y-%m-%d\") -&gt; str | None:\n    \"\"\"Parse and reformat a date string.\"\"\"\n    from datetime import datetime\n\n    value = ctx.node.get(field)\n    if not value:\n        return None\n\n    try:\n        dt = datetime.strptime(value, format)\n        return dt.isoformat()\n    except ValueError:\n        return None",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#built-in-transforms-as-reference",
    "href": "docs/custom-transforms.html#built-in-transforms-as-reference",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "Built-in Transforms as Reference",
    "text": "Built-in Transforms as Reference\netielle includes these transforms that follow the same patterns:\n\nnode() - Returns ctx.node\nparent_index(depth=1) - Returns ctx.parent.index at given depth\n\nYou can use these as reference implementations for your own transforms.",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/custom-transforms.html#see-also",
    "href": "docs/custom-transforms.html#see-also",
    "title": "Custom Transforms: The @transform Decorator",
    "section": "See also",
    "text": "See also\n\nTransforms - Built-in transforms reference\nMapping Tables - Using transforms in fields",
    "crumbs": [
      "Advanced Topics",
      "Custom Transforms"
    ]
  },
  {
    "objectID": "docs/mapping.html",
    "href": "docs/mapping.html",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "",
    "text": "What you’ll learn: How to use map_to(), Field, and TempField to define your output tables, including merge policies for combining rows.\nETL context: Mapping is part of the Transform step—it defines the structure of your output tables and how rows are keyed and merged.",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#what-is-mapping",
    "href": "docs/mapping.html#what-is-mapping",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "What is Mapping?",
    "text": "What is Mapping?\nMapping defines what table rows to create at each position in your navigation. Use map_to() to emit rows with Field (output columns) and TempField (join keys).\nfrom etielle import etl, Field, TempField, get\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"name\", get(\"name\")),      # Output column\n        Field(\"email\", get(\"email\")),    # Output column\n        TempField(\"id\", get(\"id\"))       # Join key (not in output)\n    ])\n    .run()\n)",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#field-types",
    "href": "docs/mapping.html#field-types",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "Field Types",
    "text": "Field Types\n\nField(name, transform, merge=None) - Output Column\nA Field defines a column that appears in your output:\nfrom etielle import Field, get, literal\n\nField(\"name\", get(\"name\"))              # Extract from JSON\nField(\"status\", literal(\"active\"))      # Constant value\nField(\"count\", literal(1), merge=AddPolicy())  # With merge policy\n\n\nTempField(name, transform) - Join Key\nA TempField is used for row identification and relationships, but is NOT included in the output:\nfrom etielle import TempField, get, get_from_parent\n\nTempField(\"id\", get(\"id\"))              # Primary key\nTempField(\"user_id\", get_from_parent(\"id\"))  # Foreign key for linking\nWhy TempField?\n\nDefines the unique key for each row (like a primary key)\nUsed with join_on to merge rows from different paths\nUsed with link_to() to establish relationships\nKeeps your output clean by excluding internal keys",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#basic-mapping",
    "href": "docs/mapping.html#basic-mapping",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "Basic Mapping",
    "text": "Basic Mapping\n\nSimple Table Emission\n\nfrom etielle import etl, Field, TempField, get\nimport json\n\ndata = {\"users\": [\n    {\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"alice@example.com\"},\n    {\"id\": \"u2\", \"name\": \"Bob\", \"email\": \"bob@example.com\"}\n]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"name\", get(\"name\")),\n        Field(\"email\", get(\"email\")),\n        TempField(\"id\", get(\"id\"))  # Row key\n    ])\n    .run()\n)\n\n# Access results\nfor key, row in result.tables[\"users\"].items():\n    print(f\"Key: {key}, Row: {row}\")\n\nKey: ('__auto_0__',), Row: {'name': 'Alice', 'email': 'alice@example.com', 'id': '__auto_0__'}\nKey: ('__auto_1__',), Row: {'name': 'Bob', 'email': 'bob@example.com', 'id': '__auto_1__'}\n\n\n\n\nTyped Output with Model Classes\nPass a model class instead of a string to get typed output:\n\nfrom pydantic import BaseModel\nfrom etielle import etl, Field, TempField, get\n\nclass User(BaseModel):\n    name: str\n    email: str\n\ndata = {\"users\": [{\"id\": \"u1\", \"name\": \"Alice\", \"email\": \"alice@example.com\"}]}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[  # Pass model class\n        Field(\"name\", get(\"name\")),\n        Field(\"email\", get(\"email\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\n# Access by model class\nuser = list(result.tables[User].values())[0]\nprint(f\"Type: {type(user).__name__}, Name: {user.name}\")\n\nType: User, Name: Alice\n\n\n\n\nSupported Model Types\netielle auto-detects the model type and uses the appropriate builder:\n\n\n\n\n\n\n\n\nModel Type\nDetection\nBuilder Used\n\n\n\n\nPydantic\nissubclass(cls, BaseModel)\nPydanticBuilder\n\n\nSQLAlchemy/SQLModel\nHas __tablename__ and __mapper__\nConstructorBuilder\n\n\nTypedDict\nis_typeddict(cls)\nTypedDictBuilder\n\n\nDataclass/Other\nDefault\nConstructorBuilder\n\n\nString\ntable=\"name\"\nPlain dict output",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#row-merging-with-join_on",
    "href": "docs/mapping.html#row-merging-with-join_on",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "Row Merging with join_on",
    "text": "Row Merging with join_on\nWhen multiple map_to() calls emit to the same table, rows with matching keys are merged.\n\nFirst Emission (No join_on needed)\nThe first map_to() for a table uses TempField values as the row key:\n.map_to(table=\"users\", fields=[\n    Field(\"name\", get(\"name\")),\n    TempField(\"id\", get(\"id\"))  # Defines the key\n])\n\n\nSubsequent Emissions (Require join_on)\nLater map_to() calls to the same table must specify join_on:\n.map_to(table=\"users\", join_on=[\"id\"], fields=[  # Must specify join_on\n    Field(\"email\", get(\"email\")),\n    TempField(\"id\", get(\"user_id\"))  # Must produce matching key\n])\n\n\nComplete Merge Example\n\nfrom etielle import etl, Field, TempField, get\nimport json\n\ndata = {\n    \"users\": [{\"id\": \"u1\", \"name\": \"Alice\"}],\n    \"profiles\": [{\"user_id\": \"u1\", \"email\": \"alice@example.com\", \"bio\": \"Developer\"}]\n}\n\nresult = (\n    etl(data)\n    # First emission: basic user data\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"id\", get(\"id\")),\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))  # Key: (\"u1\",)\n    ])\n\n    # Second emission: profile data (merged by id)\n    .goto_root()\n    .goto(\"profiles\").each()\n    .map_to(table=\"users\", join_on=[\"id\"], fields=[\n        Field(\"email\", get(\"email\")),\n        Field(\"bio\", get(\"bio\")),\n        TempField(\"id\", get(\"user_id\"))  # Key: (\"u1\",) - matches!\n    ])\n    .run()\n)\n\nuser = list(result.tables[\"users\"].values())[0]\nprint(json.dumps(user, indent=2))  # Has id, name, email, AND bio\n\n{\n  \"name\": \"Alice\",\n  \"id\": \"__auto_0__\"\n}",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#merge-policies",
    "href": "docs/mapping.html#merge-policies",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "Merge Policies",
    "text": "Merge Policies\nBy default, when merging rows, the last value wins. Merge policies change this behavior:\n\n\n\nPolicy\nBehavior\nUse Case\n\n\n\n\nAddPolicy()\nSum numbers\nCounters, totals\n\n\nAppendPolicy()\nAppend single item to list\nCollecting tags\n\n\nExtendPolicy()\nExtend list with another list\nMerging lists\n\n\nMinPolicy()\nKeep minimum value\nEarliest date\n\n\nMaxPolicy()\nKeep maximum value\nLatest date\n\n\nFirstNonNullPolicy()\nKeep first non-null\nFallback defaults\n\n\n\n\nUsing Merge Policies\n\nfrom etielle import etl, Field, TempField, get, literal\nfrom etielle import AddPolicy\nimport json\n\ndata = {\n    \"transactions\": [\n        {\"user_id\": \"u1\", \"amount\": 100},\n        {\"user_id\": \"u1\", \"amount\": 50},\n        {\"user_id\": \"u2\", \"amount\": 75}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"transactions\").each()\n    .map_to(table=\"totals\", fields=[\n        Field(\"total\", get(\"amount\"), merge=AddPolicy()),  # Sum amounts\n        Field(\"count\", literal(1), merge=AddPolicy()),     # Count transactions\n        TempField(\"user_id\", get(\"user_id\"))\n    ])\n    .run()\n)\n\nfor key, row in result.tables[\"totals\"].items():\n    print(f\"User {key}: {row}\")\n\nUser ('__auto_0__',): {'total': 100, 'count': 1}\nUser ('__auto_1__',): {'total': 50, 'count': 1}\nUser ('__auto_2__',): {'total': 75, 'count': 1}\n\n\n\n\nCollecting Values with AppendPolicy\n\nfrom etielle import etl, Field, TempField, get\nfrom etielle import AppendPolicy\nimport json\n\ndata = {\n    \"tags\": [\n        {\"item_id\": \"i1\", \"tag\": \"featured\"},\n        {\"item_id\": \"i1\", \"tag\": \"sale\"},\n        {\"item_id\": \"i2\", \"tag\": \"new\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"tags\").each()\n    .map_to(table=\"items\", fields=[\n        Field(\"tags\", get(\"tag\"), merge=AppendPolicy()),  # Collect into list\n        TempField(\"id\", get(\"item_id\"))\n    ])\n    .run()\n)\n\nfor key, row in result.tables[\"items\"].items():\n    print(f\"Item {key}: {row}\")\n\nItem ('__auto_0__',): {'tags': ['featured']}\nItem ('__auto_1__',): {'tags': ['sale']}\nItem ('__auto_2__',): {'tags': ['new']}",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#result-structure",
    "href": "docs/mapping.html#result-structure",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "Result Structure",
    "text": "Result Structure\n\nAccessing Tables\nresult = pipeline.run()\n\n# By string name\nusers = result.tables[\"users\"]  # Dict[tuple, dict]\n\n# By model class (if used)\nusers = result.tables[User]     # Dict[tuple, User]\n\n# Iterate\nfor key, row in result.tables[\"users\"].items():\n    print(f\"Key: {key}, Row: {row}\")\n\n\nRow Keys\nRows are keyed by tuples derived from TempField values:\n# Single TempField\nTempField(\"id\", get(\"id\"))  # Key: (\"u1\",)\n\n# Multiple TempFields (composite key)\nTempField(\"user_id\", get(\"user_id\"))\nTempField(\"post_id\", get(\"post_id\"))  # Key: (\"u1\", \"p1\")\n\n\nChecking Errors\nif result.errors:\n    for table_name, table_errors in result.errors.items():\n        for key, messages in table_errors.items():\n            print(f\"{table_name}[{key}]: {messages}\")",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#map_to-reference",
    "href": "docs/mapping.html#map_to-reference",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "map_to() Reference",
    "text": "map_to() Reference\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ntable\nstr or type\nTable name or model class\n\n\nfields\nlist[Field \\| TempField]\nField definitions\n\n\njoin_on\nlist[str] or None\nField names for row merging (required for 2nd+ emission)\n\n\nerrors\n\"collect\" or \"fail_fast\" or None\nOverride error handling for this table",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#best-practices",
    "href": "docs/mapping.html#best-practices",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways Include a TempField\nEvery map_to() should have at least one TempField to define the row key:\n# Good: Has a key\n.map_to(table=\"users\", fields=[\n    Field(\"name\", get(\"name\")),\n    TempField(\"id\", get(\"id\"))  # Row key\n])\n\n# Bad: No key (rows can't be uniquely identified)\n.map_to(table=\"users\", fields=[\n    Field(\"name\", get(\"name\"))\n])\n\n\nUse Meaningful Key Names\nChoose TempField names that match your mental model:\n# Good: Clear what each key represents\nTempField(\"user_id\", get_from_parent(\"id\"))\nTempField(\"post_id\", get(\"id\"))\n\n# Less clear\nTempField(\"key1\", get_from_parent(\"id\"))\nTempField(\"key2\", get(\"id\"))\n\n\nConsider Output vs. Keys\nDecide what goes in output (Field) vs. what’s just for joining (TempField):\n# If you need the ID in output AND as a key:\nField(\"id\", get(\"id\"))\nTempField(\"id\", get(\"id\"))  # Same value, different purpose\n\n# If ID is only for joining:\nTempField(\"id\", get(\"id\"))  # Won't appear in output",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/mapping.html#see-also",
    "href": "docs/mapping.html#see-also",
    "title": "Mapping Tables: Fields and Output Structure",
    "section": "See also",
    "text": "See also\n\nNavigation - Positioning before mapping\nTransforms - Computing field values\nRelationships - Linking tables with link_to()\nError Handling - Handling validation errors",
    "crumbs": [
      "Core Concepts",
      "Mapping Tables"
    ]
  },
  {
    "objectID": "docs/error-handling.html",
    "href": "docs/error-handling.html",
    "title": "Error Handling: Managing Validation Errors",
    "section": "",
    "text": "What you’ll learn: How to handle validation errors in etielle pipelines, including error modes and accessing error details.\nETL context: Error handling ensures data quality during the Transform step by catching and reporting validation issues.",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#error-modes",
    "href": "docs/error-handling.html#error-modes",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Error Modes",
    "text": "Error Modes\netielle supports two error handling modes:\n\n\n\nMode\nBehavior\n\n\n\n\n\"collect\" (default)\nCollect all errors and continue processing\n\n\n\"fail_fast\"\nRaise immediately on first error\n\n\n\n\nCollect Mode (Default)\nCollect all errors and inspect them after run():\n\nfrom etielle import etl, Field, TempField, get\nimport json\n\ndata = {\n    \"users\": [\n        {\"id\": \"u1\", \"name\": \"Alice\"},\n        {\"id\": None, \"name\": \"Bob\"},  # Invalid: null ID\n        {\"id\": \"u3\", \"name\": \"Carol\"}\n    ]\n}\n\nresult = (\n    etl(data, errors=\"collect\")  # Default mode\n    .goto(\"users\").each()\n    .map_to(table=\"users\", fields=[\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))  # Will have None for Bob\n    ])\n    .run()\n)\n\nprint(f\"Rows extracted: {len(result.tables['users'])}\")\nprint(f\"Has errors: {bool(result.errors)}\")\n\nRows extracted: 3\nHas errors: False\n\n\n\n\nFail Fast Mode\nStop processing immediately on first error:\n\nfrom etielle import etl, Field, TempField, get\n\ndata = {\"users\": [{\"id\": None, \"name\": \"Bob\"}]}\n\ntry:\n    result = (\n        etl(data, errors=\"fail_fast\")\n        .goto(\"users\").each()\n        .map_to(table=\"users\", fields=[\n            Field(\"name\", get(\"name\")),\n            TempField(\"id\", get(\"id\"))\n        ])\n        .run()\n    )\nexcept ValueError as e:\n    print(f\"Pipeline failed: {e}\")",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#accessing-errors",
    "href": "docs/error-handling.html#accessing-errors",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Accessing Errors",
    "text": "Accessing Errors\n\nError Structure\nErrors are keyed by table name, then by row key:\nresult.errors = {\n    \"table_name\": {\n        (\"row\", \"key\"): [\"error message 1\", \"error message 2\"],\n        (\"another\", \"key\"): [\"error message\"]\n    }\n}\n\n\nInspecting Errors\n\nfrom etielle import etl, Field, TempField, get\nfrom pydantic import BaseModel, field_validator\n\nclass User(BaseModel):\n    name: str\n\n    @field_validator(\"name\")\n    @classmethod\n    def name_not_empty(cls, v):\n        if not v or not v.strip():\n            raise ValueError(\"Name cannot be empty\")\n        return v\n\ndata = {\n    \"users\": [\n        {\"id\": \"u1\", \"name\": \"Alice\"},\n        {\"id\": \"u2\", \"name\": \"\"},      # Invalid: empty name\n        {\"id\": \"u3\", \"name\": \"Carol\"}\n    ]\n}\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[\n        Field(\"name\", get(\"name\")),\n        TempField(\"id\", get(\"id\"))\n    ])\n    .run()\n)\n\n# Check for errors\nif result.errors:\n    for table_name, table_errors in result.errors.items():\n        for row_key, messages in table_errors.items():\n            print(f\"{table_name}[{row_key}]: {messages}\")\n\nuser[('__auto_1__',)]: [\"table=user key=('__auto_1__',) 1 validation error for User\\nname\\n  Value error, Name cannot be empty [type=value_error, input_value='', input_type=str]\\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error\"]",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#per-table-error-mode",
    "href": "docs/error-handling.html#per-table-error-mode",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Per-Table Error Mode",
    "text": "Per-Table Error Mode\nOverride the pipeline-level error mode for specific tables:\nresult = (\n    etl(data, errors=\"collect\")  # Default for pipeline\n\n    .goto(\"critical_data\").each()\n    .map_to(table=CriticalModel, errors=\"fail_fast\", fields=[...])  # Override\n\n    .goto_root()\n    .goto(\"optional_data\").each()\n    .map_to(table=OptionalModel, errors=\"collect\", fields=[...])  # Explicit collect\n\n    .run()\n)",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#common-error-scenarios",
    "href": "docs/error-handling.html#common-error-scenarios",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Common Error Scenarios",
    "text": "Common Error Scenarios\n\nMissing Required Fields\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    id: str      # Required\n    name: str    # Required\n    email: str | None = None  # Optional\n\n# If data is missing \"name\", Pydantic will raise ValidationError\n# In collect mode: error is recorded, row is skipped\n# In fail_fast mode: pipeline raises immediately\n\n\nType Validation Errors\nclass Product(BaseModel):\n    id: str\n    price: float  # Expects a number\n\n# If JSON has \"price\": \"not a number\", validation fails\n\n\nTransform Errors\n@transform\ndef parse_date(ctx: Context, field: str) -&gt; str:\n    value = ctx.node.get(field)\n    # If this raises an exception, it becomes an error\n    return datetime.strptime(value, \"%Y-%m-%d\").isoformat()",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#error-handling-patterns",
    "href": "docs/error-handling.html#error-handling-patterns",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Error Handling Patterns",
    "text": "Error Handling Patterns\n\nContinue on Error\nProcess what you can, report what failed:\nresult = etl(data, errors=\"collect\").goto(...).map_to(...).run()\n\n# Process successful rows\nfor key, row in result.tables[\"users\"].items():\n    save_to_database(row)\n\n# Report failed rows\nif result.errors:\n    log_errors(result.errors)\n    send_alert(f\"Failed to process {len(result.errors['users'])} users\")\n\n\nAll-or-Nothing\nFail the entire batch if any row fails:\nresult = etl(data, errors=\"collect\").goto(...).map_to(...).run()\n\nif result.errors:\n    raise ValueError(f\"Batch failed with {sum(len(e) for e in result.errors.values())} errors\")\n\n# Only proceed if no errors\nwith Session(engine) as session:\n    for row in result.tables[\"users\"].values():\n        session.add(row)\n    session.commit()\n\n\nPartial Success with Reporting\nresult = etl(data).goto(...).map_to(...).run()\n\nsuccessful = list(result.tables[\"users\"].values())\nfailed_keys = list(result.errors.get(\"users\", {}).keys()) if result.errors else []\n\nprint(f\"Processed: {len(successful)} successful, {len(failed_keys)} failed\")\n\n# Save successful rows\nfor row in successful:\n    save(row)\n\n# Queue failed rows for retry\nfor key in failed_keys:\n    queue_for_retry(key)\n\n\nValidation Before Database\nwith Session(engine) as session:\n    result = (\n        etl(data)\n        .goto(\"users\").each()\n        .map_to(table=User, fields=[...])\n        .load(session)\n        .run()\n    )\n\n    if result.errors:\n        # Don't commit if there are errors\n        session.rollback()\n        raise ValueError(\"Validation errors occurred\")\n\n    session.commit()",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#error-types",
    "href": "docs/error-handling.html#error-types",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Error Types",
    "text": "Error Types\n\nUpdate Errors\nErrors that occur during field extraction or transformation:\n\nTransform returns an invalid value\nRequired field is missing\nType coercion fails\n\n\n\nFinalize Errors\nErrors that occur when building the final instance:\n\nPydantic validation fails\nModel constructor raises\nMissing required fields after all emissions",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#best-practices",
    "href": "docs/error-handling.html#best-practices",
    "title": "Error Handling: Managing Validation Errors",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways Check for Errors\nresult = pipeline.run()\n\n# Don't assume success\nif result.errors:\n    handle_errors(result.errors)\n\n\nLog Error Details\nimport logging\n\nif result.errors:\n    for table, errors in result.errors.items():\n        for key, messages in errors.items():\n            logging.error(f\"Validation failed for {table}[{key}]\", extra={\n                \"table\": table,\n                \"key\": key,\n                \"errors\": messages\n            })\n\n\nUse Fail Fast for Critical Data\n# Critical configuration - fail immediately\nconfig_result = etl(config_data, errors=\"fail_fast\")...run()\n\n# User content - collect and report\ncontent_result = etl(user_data, errors=\"collect\")...run()\n\n\nGraceful Degradation\nresult = etl(data).goto(...).map_to(...).run()\n\nif result.errors:\n    # Use partial results\n    users = result.tables.get(\"users\", {})\n    logging.warning(f\"Partial load: {len(users)} users, {len(result.errors.get('users', {}))} errors\")\nelse:\n    users = result.tables[\"users\"]",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/error-handling.html#see-also",
    "href": "docs/error-handling.html#see-also",
    "title": "Error Handling: Managing Validation Errors",
    "section": "See also",
    "text": "See also\n\nMapping Tables - Where validation occurs\nDatabase Loading - Error handling during persistence\nCustom Transforms - Error handling in transforms",
    "crumbs": [
      "Advanced Topics",
      "Error Handling"
    ]
  },
  {
    "objectID": "docs/introduction-to-etl.html",
    "href": "docs/introduction-to-etl.html",
    "title": "Introduction to ETL",
    "section": "",
    "text": "What you’ll learn: What ETL (Extract, Transform, Load) means, why it’s important, how it relates to working with JSON data from APIs, and how etielle’s fluent API supports each ETL step.",
    "crumbs": [
      "Getting Started",
      "Introduction to ETL"
    ]
  },
  {
    "objectID": "docs/introduction-to-etl.html#what-is-etl",
    "href": "docs/introduction-to-etl.html#what-is-etl",
    "title": "Introduction to ETL",
    "section": "What is ETL?",
    "text": "What is ETL?\nETL stands for Extract, Transform, Load—the three fundamental steps for moving data from one system to another. Understanding these steps is key to understanding how etielle works:\n\n\n\n\n\n\n\n\nETL Step\nWhat it means\nHow etielle does it\n\n\n\n\nExtract\nNavigate and pull data from a source\ngoto() and each() navigate nested JSON\n\n\nTransform\nReshape and format the data\nTransforms, Field, and TempField reshape data\n\n\nLoad\nInsert data into the target system\nload(session).run() persists to database\n\n\n\nETL is a core concept in data engineering and has been used for decades to move data between systems. etielle brings declarative ETL to Python for JSON-to-relational transformations.",
    "crumbs": [
      "Getting Started",
      "Introduction to ETL"
    ]
  },
  {
    "objectID": "docs/introduction-to-etl.html#etl-in-action-a-simple-example",
    "href": "docs/introduction-to-etl.html#etl-in-action-a-simple-example",
    "title": "Introduction to ETL",
    "section": "ETL in Action: A Simple Example",
    "text": "ETL in Action: A Simple Example\nImagine you’re building an app that tracks GitHub repositories. Here’s how each ETL step works with etielle’s fluent API:\n\n1. Extract: Navigate the JSON structure\nGoal: Pull a subset of data from GitHub’s API (returns nested JSON with repositories, contributors, commits)\nHow etielle does it: goto() and each() define how to walk through the JSON:\nfrom etielle import etl, Field, TempField, get\n\nresult = (\n    etl(github_data)\n    # Extract: Navigate to repositories array\n    .goto(\"repositories\").each()\n    .map_to(table=\"repos\", fields=[...])\n\n    # Extract: Navigate to commits nested inside each repo\n    .goto(\"commits\").each()\n    .map_to(table=\"commits\", fields=[...])\n\n    .run()\n)\nKey concept: goto() tells etielle where to go in the JSON, and each() tells it to iterate through items. They handle the Extract step.\nLearn more: Navigation\n\n\n2. Transform: Reshape data in memory\nGoal: Extract specific fields, link parent-child records, format values, build table structures\nHow etielle does it: Multiple features work together to transform data:\n\nField-level transforms extract and modify individual values: ```python from etielle import get, get_from_parent, concat, literal\n# Transform: Extract a field from the current item repo_name = get(“name”)\n# Transform: Get a field from the parent (for relationship linking) parent_repo_id = get_from_parent(“id”)\n# Transform: Combine values (e.g. “123” -&gt; “repo_123”) full_id = concat(literal(“repo_”), get(“id”)) ```\nTable-level transforms via map_to() define output structure: ```python from etielle import etl, Field, TempField, get\n( etl(data) .goto(“repositories”).each() .map_to(table=“repositories”, fields=[ Field(“id”, get(“id”)), # Output column Field(“name”, get(“name”)), # Output column Field(“url”, get(“url”)), # Output column TempField(“id”, get(“id”)) # Join key (not in output) ]) ) ```\nRelationship transforms link records together with link_to(): ```python from etielle import etl, Field, TempField, get, get_from_parent\n( etl(data) .goto(“repositories”).each() .map_to(table=Repository, fields=[…])\n .goto(\"commits\").each()\n .map_to(table=Commit, fields=[\n     Field(\"message\", get(\"message\")),\n     TempField(\"repo_id\", get_from_parent(\"id\"))\n ])\n .link_to(Repository, by={\"repo_id\": \"id\"})  # Link commits to repos\n) ```\n\nKey concept: Transforms, fields, and relationships are all processed in-memory, before any database persistence. The fluent API chains these operations naturally.\nLearn more: Transforms, Mapping Tables, Relationships\n\n\n3. Load: Persist to database\nGoal: Insert the transformed data into PostgreSQL\nHow etielle does it: load(session).run() persists the in-memory data:\nfrom etielle import etl, Field, TempField, get\n\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[...])\n    .load(session)   # Configure database session\n    .run()           # Execute and persist\n)\n\nsession.commit()     # You control the transaction\nKey concept: The Load step is optional and happens via load(). Without it, etielle just transforms JSON to in-memory Python objects.\nLearn more: Database Loading\n\nThe full ETL flow with etielle:\n\nExtract (goto(), each()): “Go to repositories, iterate through each one”\nTransform (transforms + Field + link_to()): “Get the name field, format the id, build table rows, link children to parents”\nLoad (load().run()): “Persist the in-memory objects to PostgreSQL” (optional)\n\nWithout etielle, you’d write nested loops, manual field extraction, and explicit inserts. With etielle, you declare the mapping once, and the library handles the rest.",
    "crumbs": [
      "Getting Started",
      "Introduction to ETL"
    ]
  },
  {
    "objectID": "docs/introduction-to-etl.html#the-three-pillars-of-etielle-etl",
    "href": "docs/introduction-to-etl.html#the-three-pillars-of-etielle-etl",
    "title": "Introduction to ETL",
    "section": "The Three Pillars of etielle ETL",
    "text": "The Three Pillars of etielle ETL\nTo summarize, etielle implements ETL through three core feature groups:\n\n1. Extract: Navigation\nWhat it does: Navigate nested JSON structures and iterate over items\nKey methods: - goto(path): Navigate to a nested location - each(): Iterate over list items or dict key-value pairs - goto_root(index): Switch between multiple JSON roots\nExample:\n(\n    etl(data)\n    .goto(\"users\").each()           # Extract: Navigate and iterate\n    .goto(\"posts\").each()           # Extract: Nested iteration\n    .map_to(...)\n)\nLearn more: Navigation\n\n\n2. Transform: Fields, Transforms, and Relationships\nWhat it does: Reshape data in memory from nested JSON to structured tables/objects\nKey features:\nTransforms (value extraction): - get(): Get field from current node - get_from_parent(): Get field from ancestor (for relationships) - concat(), format_id(): Format and combine values - coalesce(): Provide fallback values\nField types (table structure): - Field(name, transform): Output column - TempField(name, transform): Join key only (not in output) - merge policies: Sum, append, min/max when merging rows\nRelationships: - link_to(Parent, by={...}): Link child records to parents\nExample:\n.map_to(table=Post, fields=[\n    Field(\"title\", get(\"title\")),\n    Field(\"user_id\", get_from_parent(\"id\")),    # Link to parent\n    TempField(\"id\", get(\"id\"))                   # Join key\n])\n.link_to(User, by={\"user_id\": \"id\"})            # Bind relationship\nLearn more: Transforms, Mapping Tables, Relationships\n\n\n3. Load: Database Persistence\nWhat it does: Persist the in-memory transformed data to a database (optional)\nKey features: - load(session): Configure database session - run(): Execute pipeline and flush to database - One-shot flushing for performance - You control the transaction (commit/rollback)\nExample:\nresult = (\n    etl(data)\n    .goto(\"users\").each()\n    .map_to(table=User, fields=[...])\n    .load(session)\n    .run()\n)\n\nsession.commit()  # You control the transaction\nNote: This step is optional! You can use etielle just for in-memory JSON transformation without database persistence.\nLearn more: Database Loading",
    "crumbs": [
      "Getting Started",
      "Introduction to ETL"
    ]
  },
  {
    "objectID": "docs/introduction-to-etl.html#next-steps",
    "href": "docs/introduction-to-etl.html#next-steps",
    "title": "Introduction to ETL",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you understand ETL and how etielle implements each step, you’re ready to dive deeper:\n\nQuickstart - Jump straight into using etielle with a complete example\nNavigation - Master the Extract step: goto(), each(), goto_root()\nTransforms - Master the Transform step: value extraction and formatting\nMapping Tables - Master the Transform step: Field, TempField, merge policies\nRelationships - Master the Transform step: link_to() for linking records\nDatabase Loading - Master the Load step: load().run() for persistence",
    "crumbs": [
      "Getting Started",
      "Introduction to ETL"
    ]
  }
]